{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basline","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\nfrom datasets import load_dataset\nimport numpy as np\nimport copy\nimport time\nfrom collections import defaultdict\nimport warnings\nimport gc\nimport sys\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n# -----------------------------------------------------------------------------\n# 1. CONFIGURATION\n# -----------------------------------------------------------------------------\n\nwarnings.filterwarnings('ignore')\nlogging.set_verbosity_error()\n\n@dataclass\nclass ExperimentConfig:\n    MODEL_NAME: str = \"distilbert-base-uncased\"\n    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BATCH_SIZE: int = 16\n    MAX_LENGTH: int = 128\n    LEARNING_RATE: float = 2e-5\n    FINETUNE_EPOCHS: int = 3\n    RETAIN_SAMPLES_PER_CLASS: int = 200\n    FORGET_SCENARIOS: List[int] = None\n    SEEDS: List[int] = None\n    NUM_CLASSES: int = 4\n    \n    def __post_init__(self):\n        if self.SEEDS is None: self.SEEDS = [42, 123, 456]\n        if self.FORGET_SCENARIOS is None: self.FORGET_SCENARIOS = [50, 100, 200]\n\n# -----------------------------------------------------------------------------\n# 2. UTILS & STYLING (Q1 Paper Formatting)\n# -----------------------------------------------------------------------------\n\nclass ReportStyler:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n\n    @staticmethod\n    def print_banner(text):\n        print(f\"\\n{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.HEADER}  {text.upper()}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_scenario_header(cls_name, n, seed):\n        print(f\"\\n{ReportStyler.CYAN}┌── SCENARIO: Forget '{cls_name}' | N={n} | Seed={seed} {'─'*80}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_baseline_row(name, f_acc, r_acc, all_acc, ece, mia_auc):\n        print(f\"│ {ReportStyler.YELLOW}{name:<12}{ReportStyler.ENDC} │ F-Acc: {f_acc:.3f} │ R-Acc: {r_acc:.3f} │ All-Acc: {all_acc:.3f} │ ECE: {ece:.3f} │ MIA: {mia_auc:.3f}\")\n\n    @staticmethod\n    def print_table_header():\n        print(f\"│ {'─'*138}\")\n        print(f\"│ {ReportStyler.BOLD}{'METHOD':<10} │ {'F-ACC':<8} │ {'R-ACC':<8} │ {'ALL-ACC':<8} │ {'ECE':<8} │ {'MIA-AUC':<8} │ {'PRIVACY':<8} │ {'H-SCORE':<8} │ {'TIME':<7}{ReportStyler.ENDC}\")\n        print(f\"│ {'─'*138}\")\n\n    @staticmethod\n    def print_result_row(method, f_acc, r_acc, all_acc, ece, mia, priv, h_score, time):\n        # Color coding for quick visual analysis\n        c_f = ReportStyler.GREEN if f_acc < 0.05 else \"\"\n        c_r = ReportStyler.FAIL if r_acc < 0.7 else \"\"\n        c_h = ReportStyler.BOLD if h_score > 0.85 else \"\"\n        \n        print(f\"│ {method:<10} │ {c_f}{f_acc:<8.3f}{ReportStyler.ENDC} │ {c_r}{r_acc:<8.3f}{ReportStyler.ENDC} │ {all_acc:<8.3f} │ {ece:<8.3f} │ {mia:<8.3f} │ {priv:<8.3f} │ {c_h}{h_score:<8.3f}{ReportStyler.ENDC} │ {time:<7.1f}\")\n\ndef set_seed(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n\ndef clear_memory():\n    gc.collect(); torch.cuda.empty_cache()\n\ndef safe_deepcopy_model(model):\n    \"\"\"Deepcopy via CPU to prevent VRAM spikes.\"\"\"\n    clear_memory()\n    device = next(model.parameters()).device\n    model_cpu = model.to('cpu')\n    model_copy = copy.deepcopy(model_cpu)\n    model.to(device) # Restore original\n    return model_copy\n\n# -----------------------------------------------------------------------------\n# 3. DATA & EVALUATION\n# -----------------------------------------------------------------------------\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        self.labels = labels\n    def __len__(self): return len(self.labels)\n    def __getitem__(self, idx):\n        return {'input_ids': self.encodings['input_ids'][idx], \n                'attention_mask': self.encodings['attention_mask'][idx], \n                'labels': torch.tensor(self.labels[idx], dtype=torch.long)}\n\nclass Evaluator:\n    def __init__(self, device, config):\n        self.device = device\n        self.config = config\n        \n    def get_full_metrics(self, model, loaders, forget_class):\n        \"\"\"Returns comprehensive dictionary of all metrics for the paper\"\"\"\n        model.eval()\n        model.to(self.device)\n        \n        # 1. Classification Metrics (Accuracy, ECE)\n        all_preds, all_labels, all_probs = [], [], []\n        with torch.no_grad():\n            for batch in loaders['test_all']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                probs = F.softmax(out.logits, dim=-1)\n                all_preds.extend(torch.argmax(probs, dim=-1).cpu().numpy())\n                all_labels.extend(batch['labels'].cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n        \n        all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n        \n        # Per Class Accuracy\n        acc_per_class = {}\n        for c in range(self.config.NUM_CLASSES):\n            mask = all_labels == c\n            acc_per_class[c] = accuracy_score(all_labels[mask], all_preds[mask]) if mask.sum() > 0 else 0.0\n\n        # ECE Calculation\n        confidences = np.max(all_probs, axis=1)\n        predictions = np.argmax(all_probs, axis=1)\n        accuracies = (predictions == all_labels)\n        bins = np.linspace(0, 1, 11)\n        ece = 0.0\n        for i in range(10):\n            mask = (confidences >= bins[i]) & (confidences < bins[i+1])\n            if mask.sum() > 0:\n                ece += mask.sum()/len(all_labels) * abs(accuracies[mask].mean() - confidences[mask].mean())\n\n        # 2. Privacy Metrics (MIA)\n        # Using Loss-Gap Attack\n        crit = nn.CrossEntropyLoss(reduction='none')\n        f_losses, r_losses = [], []\n        with torch.no_grad():\n            for batch in loaders['forget_train']: # Members\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                f_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n            for batch in loaders['retain_test']: # Non-Members (approx)\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                r_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n        \n        # Member = 1 (Low Loss), Non-Member = 0 (High Loss)\n        # We invert loss for AUC calculation so higher score = likely member\n        y_true = np.concatenate([np.ones(len(f_losses)), np.zeros(len(r_losses))])\n        y_score = np.concatenate([-np.array(f_losses), -np.array(r_losses)])\n        try: mia_auc = roc_auc_score(y_true, y_score)\n        except: mia_auc = 0.5\n        \n        # Privacy Score: 1.0 is perfect privacy (AUC=0.5), 0.0 is total leakage (AUC=1.0 or 0.0)\n        privacy_score = 1.0 - (2 * abs(mia_auc - 0.5))\n\n        # 3. Composite Scores\n        f_acc = acc_per_class[forget_class]\n        r_acc = np.mean([acc_per_class[c] for c in range(4) if c != forget_class])\n        \n        # Efficacy: How close is F-Acc to 0? (1 - F_Acc)\n        eff = 1.0 - f_acc\n        # Preservation: Just R-Acc\n        pres = r_acc\n        # Harmonic Mean Score (The \"Paper Score\")\n        h_score = 2 * (eff * pres) / (eff + pres + 1e-6)\n\n        return {\n            'f_acc': f_acc,\n            'r_acc': r_acc,\n            'all_acc': accuracy_score(all_labels, all_preds),\n            'ece': ece,\n            'mia_auc': mia_auc,\n            'privacy_score': privacy_score,\n            'h_score': h_score\n        }\n\n# -----------------------------------------------------------------------------\n# 4. UNLEARNING METHODS\n# -----------------------------------------------------------------------------\n\nclass BaseUnlearner:\n    def __init__(self, model, device, loaders, config):\n        self.model = safe_deepcopy_model(model).to(device)\n        self.device = device\n        self.loaders = loaders\n        self.config = config\n        self.ref_model = safe_deepcopy_model(model).to(device)\n        self.ref_model.eval()\n\nclass GradientAscent(BaseUnlearner):\n    def run(self):\n        opt = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n        self.model.train()\n        for _ in range(5): # 5 Epochs\n            for batch in self.loaders['forget_train']:\n                out = self.model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device), labels=batch['labels'].to(self.device))\n                loss = out.loss\n                opt.zero_grad(); (-loss).backward(); opt.step() # Maximize loss\n            # Repair step on retain data\n            for batch in self.loaders['retain_train']:\n                out = self.model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device), labels=batch['labels'].to(self.device))\n                opt.zero_grad(); out.loss.backward(); opt.step()\n        return self.model\n\nclass NPO(BaseUnlearner):\n    def run(self):\n        opt = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n        self.model.train()\n        beta = 0.1\n        for _ in range(5):\n            for batch in self.loaders['forget_train']:\n                ids, mask, lbls = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['labels'].to(self.device)\n                out = self.model(ids, attention_mask=mask, labels=lbls)\n                with torch.no_grad(): ref_out = self.ref_model(ids, attention_mask=mask, labels=lbls)\n                loss = -F.logsigmoid(-beta * (ref_out.loss - out.loss))\n                opt.zero_grad(); loss.backward(); opt.step()\n            for batch in self.loaders['retain_train']:\n                out = self.model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device), labels=batch['labels'].to(self.device))\n                opt.zero_grad(); out.loss.backward(); opt.step()\n        return self.model\n\nclass TaskVector(BaseUnlearner):\n    def run(self):\n        # We need the pre-trained (zero-shot) model for this\n        # Assuming we can instantiate a fresh one or pass it. \n        # For simplicity in this script, we assume 'ref_model' is the Fine-tuned one.\n        # We will simulate a \"Pre-trained\" base by loading fresh.\n        base_model = AutoModelForSequenceClassification.from_pretrained(self.config.MODEL_NAME, num_labels=4).to(self.device)\n        \n        # W_new = W_pretrained + (W_finetuned - W_pretrained) - alpha * (W_finetuned - W_pretrained)\n        # Ideally: W_new = W_finetuned - alpha * (W_finetuned - W_pretrained)\n        alpha = 0.4\n        \n        with torch.no_grad():\n            for p_new, p_ft, p_base in zip(self.model.parameters(), self.ref_model.parameters(), base_model.parameters()):\n                task_vec = p_ft.data - p_base.data\n                p_new.data = p_ft.data - (alpha * task_vec)\n        return self.model\n\nclass Relabel(BaseUnlearner):\n    def run(self):\n        opt = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n        self.model.train()\n        for _ in range(3):\n            for batch in self.loaders['forget_train']:\n                ids, mask, lbls = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['labels'].to(self.device)\n                # Random labels\n                rand_lbls = torch.randint(0, 4, lbls.shape).to(self.device)\n                out = self.model(ids, attention_mask=mask, labels=rand_lbls)\n                opt.zero_grad(); out.loss.backward(); opt.step()\n            for batch in self.loaders['retain_train']:\n                out = self.model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device), labels=batch['labels'].to(self.device))\n                opt.zero_grad(); out.loss.backward(); opt.step()\n        return self.model\n\nclass Prune(BaseUnlearner):\n    def run(self):\n        # Selective pruning based on activation difference\n        self.model.eval()\n        base = self.model.distilbert.transformer\n        \n        for layer in base.layer:\n            # Hook intermediate activations (lin1 output)\n            f_acts, r_acts = [], []\n            h1 = layer.ffn.lin1.register_forward_hook(lambda m,i,o: f_acts.append(o.mean(dim=(0,1)).detach().cpu()))\n            with torch.no_grad():\n                for i, b in enumerate(self.loaders['forget_train']):\n                    if i>5: break\n                    self.model(b['input_ids'].to(self.device), attention_mask=b['attention_mask'].to(self.device))\n            h1.remove()\n            \n            h2 = layer.ffn.lin1.register_forward_hook(lambda m,i,o: r_acts.append(o.mean(dim=(0,1)).detach().cpu()))\n            with torch.no_grad():\n                for i, b in enumerate(self.loaders['retain_train']):\n                    if i>5: break\n                    self.model(b['input_ids'].to(self.device), attention_mask=b['attention_mask'].to(self.device))\n            h2.remove()\n\n            if f_acts and r_acts:\n                # Neurons that activate more for Forget than Retain\n                diff = torch.stack(f_acts).mean(0) - torch.stack(r_acts).mean(0)\n                # Prune top 10%\n                thresh = torch.quantile(diff, 0.90)\n                mask = (diff < thresh).float().to(self.device)\n                \n                # Apply mask to weights\n                with torch.no_grad():\n                    layer.ffn.lin1.weight.data *= mask.unsqueeze(1)\n                    layer.ffn.lin1.bias.data *= mask\n                    layer.ffn.lin2.weight.data *= mask.unsqueeze(0)\n        \n        # Quick repair\n        opt = torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n        self.model.train()\n        for _ in range(1):\n             for batch in self.loaders['retain_train']:\n                out = self.model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device), labels=batch['labels'].to(self.device))\n                opt.zero_grad(); out.loss.backward(); opt.step()\n        return self.model\n\n# -----------------------------------------------------------------------------\n# 5. MAIN EXECUTION\n# -----------------------------------------------------------------------------\n\ndef load_data(config, tokenizer):\n    raw = load_dataset(\"ag_news\")\n    data = defaultdict(lambda: defaultdict(list))\n    for split in ['train', 'test']:\n        for item in raw[split]:\n            data[split][item['label']].append(item['text'])\n    return data\n\ndef make_loader(texts, labels, tokenizer, batch_size, shuffle=False):\n    ds = TextClassificationDataset(texts, labels, tokenizer, 128)\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\ndef run_experiment():\n    conf = ExperimentConfig()\n    ReportStyler.print_banner(\"Q1 Unlearning Benchmark: AG News\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(conf.MODEL_NAME)\n    raw_data = load_data(conf, tokenizer)\n    evaluator = Evaluator(conf.DEVICE, conf)\n\n    final_results = []\n\n    for seed in conf.SEEDS:\n        set_seed(seed)\n        \n        for f_cls in range(conf.NUM_CLASSES): # Iterate all classes\n            class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n            target_name = class_names[f_cls]\n            \n            for n_forget in conf.FORGET_SCENARIOS:\n                ReportStyler.print_scenario_header(target_name, n_forget, seed)\n                \n                # --- Prepare Data ---\n                # Forget Train\n                f_train_txt = raw_data['train'][f_cls][:n_forget]\n                f_train_lbl = [f_cls]*n_forget\n                \n                # Retain Train\n                r_train_txt, r_train_lbl = [], []\n                for c in range(4):\n                    if c != f_cls:\n                        r_train_txt.extend(raw_data['train'][c][:conf.RETAIN_SAMPLES_PER_CLASS])\n                        r_train_lbl.extend([c]*conf.RETAIN_SAMPLES_PER_CLASS)\n                \n                # Test Sets\n                t_f_txt = raw_data['test'][f_cls][:200]\n                t_f_lbl = [f_cls]*200\n                t_r_txt, t_r_lbl = [], []\n                for c in range(4):\n                    if c != f_cls:\n                        t_r_txt.extend(raw_data['test'][c][:200])\n                        t_r_lbl.extend([c]*200)\n\n                loaders = {\n                    'forget_train': make_loader(f_train_txt, f_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                    'retain_train': make_loader(r_train_txt, r_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                    'forget_test':  make_loader(t_f_txt, t_f_lbl, tokenizer, conf.BATCH_SIZE, False),\n                    'retain_test':  make_loader(t_r_txt, t_r_lbl, tokenizer, conf.BATCH_SIZE, False),\n                    'test_all':     make_loader(t_f_txt+t_r_txt, t_f_lbl+t_r_lbl, tokenizer, conf.BATCH_SIZE, False)\n                }\n\n                # --- 1. Pre-trained Baseline ---\n                model = AutoModelForSequenceClassification.from_pretrained(conf.MODEL_NAME, num_labels=4).to(conf.DEVICE)\n                base_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n                ReportStyler.print_baseline_row(\"PRE-TRAINED\", base_metrics['f_acc'], base_metrics['r_acc'], base_metrics['all_acc'], base_metrics['ece'], base_metrics['mia_auc'])\n\n                # --- 2. Fine-tuning ---\n                model.train()\n                opt = torch.optim.AdamW(model.parameters(), lr=conf.LEARNING_RATE)\n                # Combine loaders for stable finetuning\n                combined = ConcatDataset([loaders['forget_train'].dataset, loaders['retain_train'].dataset])\n                train_l = DataLoader(combined, batch_size=conf.BATCH_SIZE, shuffle=True)\n                \n                for _ in range(conf.FINETUNE_EPOCHS):\n                    for batch in train_l:\n                        out = model(batch['input_ids'].to(conf.DEVICE), attention_mask=batch['attention_mask'].to(conf.DEVICE), labels=batch['labels'].to(conf.DEVICE))\n                        opt.zero_grad(); out.loss.backward(); opt.step()\n                \n                ft_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n                ReportStyler.print_baseline_row(\"FINE-TUNED\", ft_metrics['f_acc'], ft_metrics['r_acc'], ft_metrics['all_acc'], ft_metrics['ece'], ft_metrics['mia_auc'])\n                \n                # Check if finetuning worked\n                if ft_metrics['f_acc'] < 0.7:\n                    print(f\"{ReportStyler.FAIL}Skipping: Model failed to learn target class.{ReportStyler.ENDC}\")\n                    continue\n\n                # --- 3. Unlearning Loop ---\n                ReportStyler.print_table_header()\n                \n                methods = [\n                    (\"GA+GD\", GradientAscent),\n                    (\"NPO\", NPO),\n                    (\"TASKVEC\", TaskVector),\n                    (\"PRUNE\", Prune),\n                    (\"RELABEL\", Relabel)\n                ]\n\n                # We must use a CPU copy of the FT model as the starting point for every method\n                ft_model_cpu = safe_deepcopy_model(model).to('cpu')\n\n                for name, Cls in methods:\n                    try:\n                        # Restore fresh copy\n                        current_model = safe_deepcopy_model(ft_model_cpu).to(conf.DEVICE)\n                        \n                        t0 = time.time()\n                        unlearner = Cls(current_model, conf.DEVICE, loaders, conf)\n                        unlearned_model = unlearner.run()\n                        dur = time.time() - t0\n                        \n                        res = evaluator.get_full_metrics(unlearned_model, loaders, f_cls)\n                        \n                        ReportStyler.print_result_row(\n                            name, res['f_acc'], res['r_acc'], res['all_acc'], \n                            res['ece'], res['mia_auc'], res['privacy_score'], \n                            res['h_score'], dur\n                        )\n                        \n                        final_results.append({\n                            'seed': seed, 'n': n_forget, 'class': target_name, 'method': name, **res\n                        })\n                        \n                        del unlearned_model, unlearner\n                        clear_memory()\n                        \n                    except Exception as e:\n                         print(f\"│ {ReportStyler.FAIL}{name:<10} │ ERROR: {str(e)[:50]}{ReportStyler.ENDC}\")\n                         import traceback; traceback.print_exc()\n\n                print(f\"│ {'─'*138}\")\n                del model\n                clear_memory()\n\n    print(\"\\nBenchmark Complete. Copy these tables to your paper appendix.\")\n\nif __name__ == \"__main__\":\n    run_experiment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T14:09:51.297053Z","iopub.execute_input":"2025-12-27T14:09:51.297644Z","iopub.status.idle":"2025-12-27T14:53:35.839409Z","shell.execute_reply.started":"2025-12-27T14:09:51.297610Z","shell.execute_reply":"2025-12-27T14:53:35.838705Z"}},"outputs":[{"name":"stdout","text":"\n\u001b[1m============================================================================================================================================\u001b[0m\n\u001b[1m\u001b[95m  Q1 UNLEARNING BENCHMARK: AG NEWS\u001b[0m\n\u001b[1m============================================================================================================================================\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ae9d97d46914f2bac54cabdd1956e28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5318e4d3e82e467aa316741bc01b7c3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e09c730a865941e98b9394503172b3f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d74e1568bbf34d949f70ec1a7efa0f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb8c4742fd443809a5a5a43b2133e2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b94c7de5bd44a6b21cad0bc2a3b4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cbf480ade0544e18582b2cbfb036b82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b286399a6c4910992858a3347befd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d005f17ff4449ea943280988febfaa"}},"metadata":{}},{"name":"stdout","text":"\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"2025-12-27 14:10:15.987723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766844616.183550      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766844616.237954      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab8a9d0620f44443b8f1a8b6b7509afb"}},"metadata":{}},{"name":"stdout","text":"│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.308 │ All-Acc: 0.231 │ ECE: 0.034 │ MIA: 0.378\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.615 │ R-Acc: 0.868 │ All-Acc: 0.805 │ ECE: 0.065 │ MIA: 0.196\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.333 │ All-Acc: 0.251 │ ECE: 0.110 │ MIA: 0.664\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.790 │ R-Acc: 0.857 │ All-Acc: 0.840 │ ECE: 0.027 │ MIA: 0.342\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.853   \u001b[0m │ 0.640    │ 0.276    │ 0.000    │ 0.000    │ \u001b[1m0.921   \u001b[0m │ 23.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.885   \u001b[0m │ 0.664    │ 0.286    │ 0.003    │ 0.006    │ \u001b[1m0.939   \u001b[0m │ 24.5   \n│ TASKVEC    │ 0.810   \u001b[0m │ 0.808   \u001b[0m │ 0.809    │ 0.415    │ 0.398    │ 0.795    │ 0.308   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.832   \u001b[0m │ 0.624    │ 0.253    │ 0.070    │ 0.140    │ \u001b[1m0.908   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.520   \u001b[0m │ 0.838   \u001b[0m │ 0.759    │ 0.189    │ 0.135    │ 0.271    │ 0.610   \u001b[0m │ 14.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.066 │ MIA: 0.667\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.880 │ R-Acc: 0.778 │ All-Acc: 0.804 │ ECE: 0.053 │ MIA: 0.610\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.800   \u001b[0m │ 0.600    │ 0.350    │ 0.000    │ 0.000    │ \u001b[1m0.889   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.847   \u001b[0m │ 0.635    │ 0.324    │ 0.000    │ 0.000    │ \u001b[1m0.917   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.905   \u001b[0m │ \u001b[91m0.697   \u001b[0m │ 0.749    │ 0.369    │ 0.764    │ 0.472    │ 0.167   \u001b[0m │ 1.5    \n│ PRUNE      │ 0.130   \u001b[0m │ 0.848   \u001b[0m │ 0.669    │ 0.199    │ 0.072    │ 0.143    │ \u001b[1m0.859   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.075   \u001b[0m │ 0.855   \u001b[0m │ 0.660    │ 0.115    │ 0.095    │ 0.190    │ \u001b[1m0.889   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.460 │ R-Acc: 0.157 │ All-Acc: 0.233 │ ECE: 0.029 │ MIA: 0.867\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.680 │ R-Acc: 0.763 │ All-Acc: 0.743 │ ECE: 0.092 │ MIA: 0.370\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.352 │ All-Acc: 0.264 │ ECE: 0.024 │ MIA: 0.421\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.870 │ R-Acc: 0.795 │ All-Acc: 0.814 │ ECE: 0.043 │ MIA: 0.617\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.782   \u001b[0m │ 0.586    │ 0.355    │ 0.000    │ 0.000    │ \u001b[1m0.877   \u001b[0m │ 23.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.792   \u001b[0m │ 0.594    │ 0.360    │ 0.001    │ 0.002    │ \u001b[1m0.884   \u001b[0m │ 24.6   \n│ TASKVEC    │ 0.730   \u001b[0m │ 0.812   \u001b[0m │ 0.791    │ 0.399    │ 0.551    │ 0.898    │ 0.405   \u001b[0m │ 1.6    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.249    │ 0.086    │ 0.171    │ \u001b[1m0.893   \u001b[0m │ 7.1    \n│ RELABEL    │ \u001b[92m0.000   \u001b[0m │ 0.820   \u001b[0m │ 0.615    │ 0.177    │ 0.115    │ 0.229    │ \u001b[1m0.901   \u001b[0m │ 14.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.080 │ R-Acc: 0.182 │ All-Acc: 0.156 │ ECE: 0.117 │ MIA: 0.581\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.840 │ R-Acc: 0.772 │ All-Acc: 0.789 │ ECE: 0.076 │ MIA: 0.877\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.326    │ 0.000    │ 0.000    │ \u001b[1m0.893   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.795   \u001b[0m │ 0.596    │ 0.344    │ 0.001    │ 0.001    │ \u001b[1m0.886   \u001b[0m │ 28.4   \n│ TASKVEC    │ 0.855   \u001b[0m │ 0.782   \u001b[0m │ 0.800    │ 0.411    │ 0.652    │ 0.695    │ 0.245   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.793   \u001b[0m │ 0.595    │ 0.281    │ 0.036    │ 0.072    │ \u001b[1m0.885   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.760   \u001b[0m │ 0.790   \u001b[0m │ 0.782    │ 0.231    │ 0.171    │ 0.343    │ 0.368   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.110 │ R-Acc: 0.313 │ All-Acc: 0.263 │ ECE: 0.010 │ MIA: 0.488\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.605 │ R-Acc: 0.882 │ All-Acc: 0.812 │ ECE: 0.039 │ MIA: 0.159\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.725 │ R-Acc: 0.050 │ All-Acc: 0.219 │ ECE: 0.054 │ MIA: 0.930\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.650 │ R-Acc: 0.828 │ All-Acc: 0.784 │ ECE: 0.058 │ MIA: 0.290\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.418 │ All-Acc: 0.315 │ ECE: 0.046 │ MIA: 0.361\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.915 │ R-Acc: 0.805 │ All-Acc: 0.833 │ ECE: 0.034 │ MIA: 0.609\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.912   \u001b[0m │ 0.684    │ 0.261    │ 0.000    │ 0.000    │ \u001b[1m0.954   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.923   \u001b[0m │ 0.693    │ 0.272    │ 0.001    │ 0.001    │ \u001b[1m0.960   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.895   \u001b[0m │ 0.790   \u001b[0m │ 0.816    │ 0.423    │ 0.616    │ 0.769    │ 0.185   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.908   \u001b[0m │ 0.681    │ 0.205    │ 0.024    │ 0.047    │ \u001b[1m0.952   \u001b[0m │ 7.1    \n│ RELABEL    │ 0.165   \u001b[0m │ 0.917   \u001b[0m │ 0.729    │ 0.060    │ 0.066    │ 0.131    │ \u001b[1m0.874   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.335 │ All-Acc: 0.251 │ ECE: 0.035 │ MIA: 0.130\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.285 │ R-Acc: 0.868 │ All-Acc: 0.723 │ ECE: 0.135 │ MIA: 0.206\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.025 │ R-Acc: 0.335 │ All-Acc: 0.258 │ ECE: 0.023 │ MIA: 0.547\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.425 │ R-Acc: 0.855 │ All-Acc: 0.748 │ ECE: 0.101 │ MIA: 0.331\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.337 │ All-Acc: 0.253 │ ECE: 0.024 │ MIA: 0.000\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.755 │ R-Acc: 0.868 │ All-Acc: 0.840 │ ECE: 0.034 │ MIA: 0.507\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.903   \u001b[0m │ 0.677    │ 0.288    │ 0.000    │ 0.000    │ \u001b[1m0.949   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.923   \u001b[0m │ 0.693    │ 0.281    │ 0.000    │ 0.000    │ \u001b[1m0.960   \u001b[0m │ 28.4   \n│ TASKVEC    │ 0.650   \u001b[0m │ 0.883   \u001b[0m │ 0.825    │ 0.433    │ 0.465    │ 0.930    │ 0.501   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.010   \u001b[0m │ 0.905   \u001b[0m │ 0.681    │ 0.220    │ 0.053    │ 0.106    │ \u001b[1m0.946   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.245   \u001b[0m │ 0.903   \u001b[0m │ 0.739    │ 0.124    │ 0.086    │ 0.172    │ 0.823   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.100 │ R-Acc: 0.327 │ All-Acc: 0.270 │ ECE: 0.001 │ MIA: 0.628\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.775 │ R-Acc: 0.803 │ All-Acc: 0.796 │ ECE: 0.058 │ MIA: 0.292\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.812   \u001b[0m │ 0.609    │ 0.313    │ 0.000    │ 0.000    │ \u001b[1m0.896   \u001b[0m │ 22.0   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.850   \u001b[0m │ 0.637    │ 0.306    │ 0.006    │ 0.012    │ \u001b[1m0.919   \u001b[0m │ 22.5   \n│ TASKVEC    │ 0.750   \u001b[0m │ 0.777   \u001b[0m │ 0.770    │ 0.386    │ 0.362    │ 0.724    │ 0.378   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.818   \u001b[0m │ 0.614    │ 0.268    │ 0.080    │ 0.159    │ \u001b[1m0.900   \u001b[0m │ 6.6    \n│ RELABEL    │ 0.595   \u001b[0m │ 0.858   \u001b[0m │ 0.792    │ 0.100    │ 0.155    │ 0.309    │ 0.550   \u001b[0m │ 13.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.015 │ R-Acc: 0.422 │ All-Acc: 0.320 │ ECE: 0.055 │ MIA: 0.349\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.805 │ R-Acc: 0.832 │ All-Acc: 0.825 │ ECE: 0.060 │ MIA: 0.551\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.853   \u001b[0m │ 0.640    │ 0.300    │ 0.000    │ 0.000    │ \u001b[1m0.921   \u001b[0m │ 23.7   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.863   \u001b[0m │ 0.647    │ 0.311    │ 0.001    │ 0.003    │ \u001b[1m0.927   \u001b[0m │ 24.5   \n│ TASKVEC    │ 0.775   \u001b[0m │ 0.832   \u001b[0m │ 0.818    │ 0.429    │ 0.425    │ 0.849    │ 0.354   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.857   \u001b[0m │ 0.642    │ 0.242    │ 0.053    │ 0.106    │ \u001b[1m0.923   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.385   \u001b[0m │ 0.870   \u001b[0m │ 0.749    │ 0.123    │ 0.105    │ 0.211    │ 0.721   \u001b[0m │ 14.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.025 │ R-Acc: 0.270 │ All-Acc: 0.209 │ ECE: 0.061 │ MIA: 0.342\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.830 │ R-Acc: 0.810 │ All-Acc: 0.815 │ ECE: 0.061 │ MIA: 0.795\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.838   \u001b[0m │ 0.629    │ 0.310    │ 0.000    │ 0.000    │ \u001b[1m0.912   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.820   \u001b[0m │ 0.615    │ 0.327    │ 0.001    │ 0.001    │ \u001b[1m0.901   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.845   \u001b[0m │ 0.763   \u001b[0m │ 0.784    │ 0.382    │ 0.721    │ 0.558    │ 0.258   \u001b[0m │ 1.5    \n│ PRUNE      │ 0.100   \u001b[0m │ 0.875   \u001b[0m │ 0.681    │ 0.169    │ 0.044    │ 0.087    │ \u001b[1m0.887   \u001b[0m │ 7.1    \n│ RELABEL    │ 0.175   \u001b[0m │ 0.847   \u001b[0m │ 0.679    │ 0.123    │ 0.100    │ 0.200    │ 0.836   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.292 │ All-Acc: 0.220 │ ECE: 0.052 │ MIA: 0.405\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.715 │ R-Acc: 0.800 │ All-Acc: 0.779 │ ECE: 0.063 │ MIA: 0.365\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.792   \u001b[0m │ 0.594    │ 0.333    │ 0.000    │ 0.000    │ \u001b[1m0.884   \u001b[0m │ 22.1   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.820   \u001b[0m │ 0.615    │ 0.329    │ 0.001    │ 0.003    │ \u001b[1m0.901   \u001b[0m │ 22.5   \n│ TASKVEC    │ 0.330   \u001b[0m │ 0.745   \u001b[0m │ 0.641    │ 0.261    │ 0.266    │ 0.531    │ 0.706   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.792   \u001b[0m │ 0.594    │ 0.274    │ 0.053    │ 0.105    │ \u001b[1m0.884   \u001b[0m │ 6.6    \n│ RELABEL    │ 0.300   \u001b[0m │ 0.800   \u001b[0m │ 0.675    │ 0.123    │ 0.187    │ 0.374    │ 0.747   \u001b[0m │ 13.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.985 │ R-Acc: 0.008 │ All-Acc: 0.253 │ ECE: 0.032 │ MIA: 0.996\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.845 │ R-Acc: 0.808 │ All-Acc: 0.818 │ ECE: 0.038 │ MIA: 0.581\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.797   \u001b[0m │ 0.598    │ 0.324    │ 0.000    │ 0.000    │ \u001b[1m0.887   \u001b[0m │ 23.7   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.802   \u001b[0m │ 0.601    │ 0.348    │ 0.001    │ 0.002    │ \u001b[1m0.890   \u001b[0m │ 24.5   \n│ TASKVEC    │ 0.755   \u001b[0m │ 0.788   \u001b[0m │ 0.780    │ 0.399    │ 0.485    │ 0.970    │ 0.374   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.802   \u001b[0m │ 0.601    │ 0.232    │ 0.094    │ 0.188    │ \u001b[1m0.890   \u001b[0m │ 7.0    \n│ RELABEL    │ \u001b[92m0.015   \u001b[0m │ 0.797   \u001b[0m │ 0.601    │ 0.180    │ 0.127    │ 0.253    │ \u001b[1m0.881   \u001b[0m │ 14.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.620 │ R-Acc: 0.205 │ All-Acc: 0.309 │ ECE: 0.047 │ MIA: 0.902\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.950 │ R-Acc: 0.790 │ All-Acc: 0.830 │ ECE: 0.057 │ MIA: 0.957\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.778   \u001b[0m │ 0.584    │ 0.355    │ 0.000    │ 0.000    │ \u001b[1m0.875   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.782   \u001b[0m │ 0.586    │ 0.358    │ 0.000    │ 0.001    │ \u001b[1m0.877   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.915   \u001b[0m │ 0.723   \u001b[0m │ 0.771    │ 0.370    │ 0.865    │ 0.270    │ 0.152   \u001b[0m │ 1.4    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.802   \u001b[0m │ 0.601    │ 0.287    │ 0.030    │ 0.060    │ \u001b[1m0.890   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.115   \u001b[0m │ 0.792   \u001b[0m │ 0.623    │ 0.134    │ 0.165    │ 0.329    │ 0.836   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.025 │ R-Acc: 0.267 │ All-Acc: 0.206 │ ECE: 0.056 │ MIA: 0.407\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.595 │ R-Acc: 0.878 │ All-Acc: 0.807 │ ECE: 0.016 │ MIA: 0.162\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.035 │ R-Acc: 0.233 │ All-Acc: 0.184 │ ECE: 0.078 │ MIA: 0.179\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.595 │ R-Acc: 0.850 │ All-Acc: 0.786 │ ECE: 0.062 │ MIA: 0.256\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.038 │ MIA: 0.142\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.735 │ R-Acc: 0.813 │ All-Acc: 0.794 │ ECE: 0.072 │ MIA: 0.348\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.892   \u001b[0m │ 0.669    │ 0.263    │ 0.000    │ 0.000    │ \u001b[1m0.943   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.883   \u001b[0m │ 0.662    │ 0.302    │ 0.000    │ 0.000    │ \u001b[1m0.938   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.835   \u001b[0m │ 0.755   \u001b[0m │ 0.775    │ 0.380    │ 0.590    │ 0.820    │ 0.271   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.913   \u001b[0m │ 0.685    │ 0.227    │ 0.024    │ 0.049    │ \u001b[1m0.955   \u001b[0m │ 7.1    \n│ RELABEL    │ \u001b[92m0.010   \u001b[0m │ 0.902   \u001b[0m │ 0.679    │ 0.102    │ 0.049    │ 0.099    │ \u001b[1m0.944   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.010 │ R-Acc: 0.330 │ All-Acc: 0.250 │ ECE: 0.025 │ MIA: 0.624\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.205 │ R-Acc: 0.885 │ All-Acc: 0.715 │ ECE: 0.142 │ MIA: 0.178\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.340 │ All-Acc: 0.255 │ ECE: 0.016 │ MIA: 0.007\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.465 │ R-Acc: 0.845 │ All-Acc: 0.750 │ ECE: 0.101 │ MIA: 0.357\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.035 │ R-Acc: 0.283 │ All-Acc: 0.221 │ ECE: 0.054 │ MIA: 0.647\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.660 │ R-Acc: 0.880 │ All-Acc: 0.825 │ ECE: 0.049 │ MIA: 0.427\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.240 │ R-Acc: 0.372 │ All-Acc: 0.339 │ ECE: 0.067 │ MIA: 0.478\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.650 │ R-Acc: 0.843 │ All-Acc: 0.795 │ ECE: 0.064 │ MIA: 0.226\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.045 │ R-Acc: 0.317 │ All-Acc: 0.249 │ ECE: 0.025 │ MIA: 0.634\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.750 │ R-Acc: 0.857 │ All-Acc: 0.830 │ ECE: 0.054 │ MIA: 0.323\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.853   \u001b[0m │ 0.640    │ 0.299    │ 0.000    │ 0.000    │ \u001b[1m0.921   \u001b[0m │ 23.6   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.843   \u001b[0m │ 0.632    │ 0.320    │ 0.001    │ 0.002    │ \u001b[1m0.915   \u001b[0m │ 24.4   \n│ TASKVEC    │ 0.710   \u001b[0m │ 0.830   \u001b[0m │ 0.800    │ 0.404    │ 0.242    │ 0.484    │ 0.430   \u001b[0m │ 1.4    \n│ PRUNE      │ \u001b[92m0.020   \u001b[0m │ 0.890   \u001b[0m │ 0.672    │ 0.193    │ 0.043    │ 0.087    │ \u001b[1m0.933   \u001b[0m │ 7.0    \n│ RELABEL    │ 0.325   \u001b[0m │ 0.873   \u001b[0m │ 0.736    │ 0.102    │ 0.102    │ 0.203    │ 0.761   \u001b[0m │ 14.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.305 │ All-Acc: 0.229 │ ECE: 0.057 │ MIA: 0.220\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.815 │ R-Acc: 0.828 │ All-Acc: 0.825 │ ECE: 0.058 │ MIA: 0.678\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.843   \u001b[0m │ 0.632    │ 0.311    │ 0.000    │ 0.000    │ \u001b[1m0.915   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.888   \u001b[0m │ 0.666    │ 0.291    │ 0.000    │ 0.000    │ \u001b[1m0.941   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.825   \u001b[0m │ 0.767   \u001b[0m │ 0.781    │ 0.394    │ 0.641    │ 0.719    │ 0.285   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.015   \u001b[0m │ 0.880   \u001b[0m │ 0.664    │ 0.212    │ 0.042    │ 0.085    │ \u001b[1m0.930   \u001b[0m │ 7.1    \n│ RELABEL    │ 0.380   \u001b[0m │ 0.855   \u001b[0m │ 0.736    │ 0.095    │ 0.112    │ 0.223    │ 0.719   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.015 │ R-Acc: 0.395 │ All-Acc: 0.300 │ ECE: 0.032 │ MIA: 0.345\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.640 │ R-Acc: 0.762 │ All-Acc: 0.731 │ ECE: 0.084 │ MIA: 0.393\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.015 │ R-Acc: 0.360 │ All-Acc: 0.274 │ ECE: 0.006 │ MIA: 0.257\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.885 │ R-Acc: 0.793 │ All-Acc: 0.816 │ ECE: 0.035 │ MIA: 0.519\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.782   \u001b[0m │ 0.586    │ 0.345    │ 0.000    │ 0.000    │ \u001b[1m0.877   \u001b[0m │ 23.6   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.783   \u001b[0m │ 0.588    │ 0.365    │ 0.002    │ 0.003    │ \u001b[1m0.879   \u001b[0m │ 24.4   \n│ TASKVEC    │ 0.560   \u001b[0m │ 0.800   \u001b[0m │ 0.740    │ 0.342    │ 0.538    │ 0.924    │ 0.568   \u001b[0m │ 1.4    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.763   \u001b[0m │ 0.573    │ 0.305    │ 0.067    │ 0.134    │ \u001b[1m0.866   \u001b[0m │ 7.0    \n│ RELABEL    │ \u001b[92m0.000   \u001b[0m │ 0.770   \u001b[0m │ 0.578    │ 0.199    │ 0.158    │ 0.315    │ \u001b[1m0.870   \u001b[0m │ 14.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 1.000 │ R-Acc: 0.000 │ All-Acc: 0.250 │ ECE: 0.044 │ MIA: 1.000\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.910 │ R-Acc: 0.788 │ All-Acc: 0.819 │ ECE: 0.032 │ MIA: 0.770\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.783   \u001b[0m │ 0.588    │ 0.336    │ 0.000    │ 0.000    │ \u001b[1m0.879   \u001b[0m │ 26.9   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.788   \u001b[0m │ 0.591    │ 0.353    │ 0.000    │ 0.000    │ \u001b[1m0.882   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.785   \u001b[0m │ 0.773   \u001b[0m │ 0.776    │ 0.386    │ 0.685    │ 0.629    │ 0.336   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.785   \u001b[0m │ 0.589    │ 0.264    │ 0.059    │ 0.118    │ \u001b[1m0.880   \u001b[0m │ 7.1    \n│ RELABEL    │ \u001b[92m0.000   \u001b[0m │ 0.800   \u001b[0m │ 0.600    │ 0.153    │ 0.127    │ 0.254    │ \u001b[1m0.889   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.322 │ All-Acc: 0.241 │ ECE: 0.057 │ MIA: 0.115\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.730 │ R-Acc: 0.847 │ All-Acc: 0.818 │ ECE: 0.034 │ MIA: 0.255\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.920   \u001b[0m │ 0.690    │ 0.271    │ 0.000    │ 0.000    │ \u001b[1m0.958   \u001b[0m │ 22.1   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.917   \u001b[0m │ 0.688    │ 0.288    │ 0.003    │ 0.006    │ \u001b[1m0.957   \u001b[0m │ 22.4   \n│ TASKVEC    │ 0.430   \u001b[0m │ 0.873   \u001b[0m │ 0.762    │ 0.382    │ 0.165    │ 0.329    │ 0.690   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.917   \u001b[0m │ 0.688    │ 0.226    │ 0.027    │ 0.053    │ \u001b[1m0.957   \u001b[0m │ 6.5    \n│ RELABEL    │ 0.210   \u001b[0m │ 0.913   \u001b[0m │ 0.738    │ 0.132    │ 0.063    │ 0.126    │ 0.847   \u001b[0m │ 13.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.038 │ MIA: 0.635\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.860 │ R-Acc: 0.802 │ All-Acc: 0.816 │ ECE: 0.030 │ MIA: 0.363\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.920   \u001b[0m │ 0.690    │ 0.267    │ 0.000    │ 0.000    │ \u001b[1m0.958   \u001b[0m │ 23.7   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.922   \u001b[0m │ 0.691    │ 0.280    │ 0.000    │ 0.000    │ \u001b[1m0.959   \u001b[0m │ 24.5   \n│ TASKVEC    │ 0.775   \u001b[0m │ 0.830   \u001b[0m │ 0.816    │ 0.443    │ 0.373    │ 0.747    │ 0.354   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.900   \u001b[0m │ 0.675    │ 0.218    │ 0.031    │ 0.062    │ \u001b[1m0.947   \u001b[0m │ 7.1    \n│ RELABEL    │ 0.405   \u001b[0m │ 0.888   \u001b[0m │ 0.767    │ 0.106    │ 0.109    │ 0.219    │ 0.713   \u001b[0m │ 14.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.313 │ All-Acc: 0.235 │ ECE: 0.031 │ MIA: 0.006\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.880 │ R-Acc: 0.778 │ All-Acc: 0.804 │ ECE: 0.063 │ MIA: 0.463\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GA+GD      │ \u001b[92m0.000   \u001b[0m │ 0.907   \u001b[0m │ 0.680    │ 0.241    │ 0.000    │ 0.000    │ \u001b[1m0.951   \u001b[0m │ 26.8   \n│ NPO        │ \u001b[92m0.000   \u001b[0m │ 0.918   \u001b[0m │ 0.689    │ 0.270    │ 0.000    │ 0.000    │ \u001b[1m0.957   \u001b[0m │ 28.5   \n│ TASKVEC    │ 0.870   \u001b[0m │ 0.782   \u001b[0m │ 0.804    │ 0.403    │ 0.541    │ 0.918    │ 0.223   \u001b[0m │ 1.5    \n│ PRUNE      │ \u001b[92m0.000   \u001b[0m │ 0.890   \u001b[0m │ 0.667    │ 0.235    │ 0.028    │ 0.055    │ \u001b[1m0.942   \u001b[0m │ 7.1    \n│ RELABEL    │ \u001b[92m0.045   \u001b[0m │ 0.895   \u001b[0m │ 0.682    │ 0.093    │ 0.068    │ 0.137    │ \u001b[1m0.924   \u001b[0m │ 16.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.445 │ R-Acc: 0.285 │ All-Acc: 0.325 │ ECE: 0.049 │ MIA: 0.747\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.240 │ R-Acc: 0.887 │ All-Acc: 0.725 │ ECE: 0.143 │ MIA: 0.185\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.033 │ MIA: 0.410\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.515 │ R-Acc: 0.852 │ All-Acc: 0.767 │ ECE: 0.093 │ MIA: 0.295\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.323 │ All-Acc: 0.242 │ ECE: 0.037 │ MIA: 0.008\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.420 │ R-Acc: 0.912 │ All-Acc: 0.789 │ ECE: 0.095 │ MIA: 0.354\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\nBenchmark Complete. Copy these tables to your paper appendix.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# New Methods","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\nfrom datasets import load_dataset\nimport numpy as np\nimport copy\nimport time\nfrom collections import defaultdict\nimport warnings\nimport gc\nfrom typing import Dict, List\nfrom dataclasses import dataclass\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings('ignore')\nlogging.set_verbosity_error()\n\nBEST_BASELINE_H_SCORE = 0.939\nBEST_BASELINE_METHOD = \"NPO\"\n\n@dataclass\nclass ExperimentConfig:\n    MODEL_NAME: str = \"distilbert-base-uncased\"\n    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BATCH_SIZE: int = 16\n    MAX_LENGTH: int = 128\n    LEARNING_RATE: float = 2e-5\n    FINETUNE_EPOCHS: int = 3\n    RETAIN_SAMPLES_PER_CLASS: int = 200\n    FORGET_SCENARIOS: List[int] = None\n    SEED: int = 42\n    NUM_CLASSES: int = 4\n    \n    def __post_init__(self):\n        if self.FORGET_SCENARIOS is None:\n            self.FORGET_SCENARIOS = [50, 100, 200]\n\n\nclass ReportStyler:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n\n    @staticmethod\n    def print_banner(text):\n        print(f\"\\n{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.HEADER}  {text.upper()}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_scenario_header(cls_name, n, seed):\n        print(f\"\\n{ReportStyler.CYAN}┌── SCENARIO: Forget '{cls_name}' | N={n} | Seed={seed} {'─'*80}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_baseline_row(name, f_acc, r_acc, all_acc, ece, mia_auc):\n        print(f\"│ {ReportStyler.YELLOW}{name:<12}{ReportStyler.ENDC} │ F-Acc: {f_acc:.3f} │ R-Acc: {r_acc:.3f} │ All-Acc: {all_acc:.3f} │ ECE: {ece:.3f} │ MIA: {mia_auc:.3f}\")\n\n    @staticmethod\n    def print_table_header():\n        print(f\"│ {'─'*138}\")\n        print(f\"│ {ReportStyler.BOLD}{'METHOD':<12} │ {'F-ACC':<8} │ {'R-ACC':<8} │ {'ALL-ACC':<8} │ {'ECE':<8} │ {'MIA-AUC':<8} │ {'PRIVACY':<8} │ {'H-SCORE':<8} │ {'TIME':<7}{ReportStyler.ENDC}\")\n        print(f\"│ {'─'*138}\")\n\n    @staticmethod\n    def print_result_row(method, f_acc, r_acc, all_acc, ece, mia, priv, h_score, elapsed):\n        c_f = ReportStyler.GREEN if f_acc < 0.05 else \"\"\n        c_r = ReportStyler.FAIL if r_acc < 0.7 else \"\"\n        c_h = ReportStyler.BOLD + ReportStyler.GREEN if h_score > BEST_BASELINE_H_SCORE else \"\"\n        print(f\"│ {method:<12} │ {c_f}{f_acc:<8.3f}{ReportStyler.ENDC} │ {c_r}{r_acc:<8.3f}{ReportStyler.ENDC} │ {all_acc:<8.3f} │ {ece:<8.3f} │ {mia:<8.3f} │ {priv:<8.3f} │ {c_h}{h_score:<8.3f}{ReportStyler.ENDC} │ {elapsed:<7.1f}\")\n\n    @staticmethod\n    def print_new_best(method, h_score, scenario):\n        print(f\"\\n{ReportStyler.BOLD}{ReportStyler.GREEN}{'★'*60}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.GREEN}  🏆 NEW SOTA! '{method}' H-Score={h_score:.4f}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.GREEN}  Beats {BEST_BASELINE_METHOD} ({BEST_BASELINE_H_SCORE:.4f}) by +{(h_score - BEST_BASELINE_H_SCORE)*100:.2f}%{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.GREEN}  Scenario: {scenario}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.GREEN}{'★'*60}{ReportStyler.ENDC}\\n\")\n\n\ndef set_seed(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef safe_deepcopy_model(model):\n    clear_memory()\n    device = next(model.parameters()).device\n    model_cpu = model.to('cpu')\n    model_copy = copy.deepcopy(model_cpu)\n    model.to(device)\n    return model_copy\n\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n\nclass Evaluator:\n    def __init__(self, device, config):\n        self.device = device\n        self.config = config\n        \n    def get_full_metrics(self, model, loaders, forget_class):\n        model.eval()\n        model.to(self.device)\n        \n        all_preds, all_labels, all_probs = [], [], []\n        with torch.no_grad():\n            for batch in loaders['test_all']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                probs = F.softmax(out.logits, dim=-1)\n                all_preds.extend(torch.argmax(probs, dim=-1).cpu().numpy())\n                all_labels.extend(batch['labels'].cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n        \n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        all_probs = np.array(all_probs)\n        \n        acc_per_class = {}\n        for c in range(self.config.NUM_CLASSES):\n            mask = all_labels == c\n            acc_per_class[c] = accuracy_score(all_labels[mask], all_preds[mask]) if mask.sum() > 0 else 0.0\n\n        confidences = np.max(all_probs, axis=1)\n        predictions = np.argmax(all_probs, axis=1)\n        accuracies = (predictions == all_labels)\n        bins = np.linspace(0, 1, 11)\n        ece = 0.0\n        for i in range(10):\n            mask = (confidences >= bins[i]) & (confidences < bins[i+1])\n            if mask.sum() > 0:\n                ece += mask.sum() / len(all_labels) * abs(accuracies[mask].mean() - confidences[mask].mean())\n\n        crit = nn.CrossEntropyLoss(reduction='none')\n        f_losses, r_losses = [], []\n        with torch.no_grad():\n            for batch in loaders['forget_train']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                f_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n            for batch in loaders['retain_test']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                r_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n        \n        y_true = np.concatenate([np.ones(len(f_losses)), np.zeros(len(r_losses))])\n        y_score = np.concatenate([-np.array(f_losses), -np.array(r_losses)])\n        try:\n            mia_auc = roc_auc_score(y_true, y_score)\n        except:\n            mia_auc = 0.5\n        \n        privacy_score = 1.0 - (2 * abs(mia_auc - 0.5))\n\n        f_acc = acc_per_class[forget_class]\n        r_acc = np.mean([acc_per_class[c] for c in range(4) if c != forget_class])\n        \n        eff = 1.0 - f_acc\n        pres = r_acc\n        h_score = 2 * (eff * pres) / (eff + pres + 1e-6)\n\n        return {\n            'f_acc': f_acc,\n            'r_acc': r_acc,\n            'all_acc': accuracy_score(all_labels, all_preds),\n            'ece': ece,\n            'mia_auc': mia_auc,\n            'privacy_score': privacy_score,\n            'h_score': h_score\n        }\n\n\nclass BaseUnlearner:\n    def __init__(self, model, device, loaders, config, forget_class):\n        self.model = safe_deepcopy_model(model).to(device)\n        self.device = device\n        self.loaders = loaders\n        self.config = config\n        self.forget_class = forget_class\n        self.ref_model = safe_deepcopy_model(model).to(device)\n        self.ref_model.eval()\n        for p in self.ref_model.parameters():\n            p.requires_grad = False\n    \n    def get_transformer_layers(self):\n        return list(self.model.distilbert.transformer.layer)\n    \n    def _kl_loss(self, logits1, logits2):\n        return F.kl_div(\n            F.log_softmax(logits1, dim=-1),\n            F.softmax(logits2, dim=-1),\n            reduction='batchmean'\n        )\n\n\n# ============================================================================\n# PROVEN CHAMPIONS (Keep these)\n# ============================================================================\n\nclass GradientConflictMinimization(BaseUnlearner):\n    \"\"\"CHAMPION - Tied SOTA at 0.933\"\"\"\n    def run(self):\n        retain_grad_signature = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                retain_grad_signature[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in retain_grad_signature and param.grad is not None:\n                    retain_grad_signature[name] += torch.sign(param.grad.data)\n        \n        for name in retain_grad_signature:\n            retain_grad_signature[name] = torch.sign(retain_grad_signature[name])\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=3e-5)\n        \n        for epoch in range(5):  # More epochs\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in retain_grad_signature and param.grad is not None:\n                            forget_grad_sign = torch.sign(param.grad.data)\n                            conflict_mask = (forget_grad_sign == retain_grad_signature[name]).float()\n                            param.grad.data *= conflict_mask\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\nclass HybridFFNGradientConflict(BaseUnlearner):\n    \"\"\"STRONG - 0.931\"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        \n        # PHASE 1: FFN Pruning\n        forget_acts = {i: [] for i in range(len(layers))}\n        retain_acts = {i: [] for i in range(len(layers))}\n        \n        self.model.eval()\n        \n        for i, layer in enumerate(layers):\n            with torch.no_grad():\n                for batch in list(self.loaders['forget_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    ffn_out = layer.ffn.lin1(h)\n                    forget_acts[i].append(ffn_out.mean(dim=(0, 1)).cpu())\n                \n                for batch in list(self.loaders['retain_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    ffn_out = layer.ffn.lin1(h)\n                    retain_acts[i].append(ffn_out.mean(dim=(0, 1)).cpu())\n        \n        with torch.no_grad():\n            for i, layer in enumerate(layers):\n                if forget_acts[i] and retain_acts[i]:\n                    f_act = torch.stack(forget_acts[i]).mean(dim=0).to(self.device)\n                    r_act = torch.stack(retain_acts[i]).mean(dim=0).to(self.device)\n                    \n                    diff = (f_act - r_act)\n                    threshold = diff.quantile(0.75)\n                    prune_mask = (diff < threshold).float()\n                    \n                    layer.ffn.lin1.weight.data *= prune_mask.unsqueeze(1)\n                    if layer.ffn.lin1.bias is not None:\n                        layer.ffn.lin1.bias.data *= prune_mask\n        \n        # PHASE 2: Gradient conflict\n        retain_grad_signature = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                retain_grad_signature[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in retain_grad_signature and param.grad is not None:\n                    retain_grad_signature[name] += torch.sign(param.grad.data)\n        \n        for name in retain_grad_signature:\n            retain_grad_signature[name] = torch.sign(retain_grad_signature[name])\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=2.5e-5)\n        \n        for epoch in range(4):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in retain_grad_signature and param.grad is not None:\n                            forget_grad_sign = torch.sign(param.grad.data)\n                            conflict_mask = (forget_grad_sign == retain_grad_signature[name]).float()\n                            param.grad.data *= conflict_mask\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\nclass AlternatingDirectionUnlearning(BaseUnlearner):\n    \"\"\"FAST - 0.922 in 4.4s\"\"\"\n    def run(self):\n        opt = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n        self.model.train()\n        \n        forget_iter = iter(self.loaders['forget_train'])\n        retain_iter = iter(self.loaders['retain_train'])\n        \n        n_steps = min(len(self.loaders['forget_train']), len(self.loaders['retain_train'])) * 5  # More steps\n        \n        for step in range(n_steps):\n            if step % 2 == 0:\n                try:\n                    batch = next(forget_iter)\n                except StopIteration:\n                    forget_iter = iter(self.loaders['forget_train'])\n                    batch = next(forget_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            else:\n                try:\n                    batch = next(retain_iter)\n                except StopIteration:\n                    retain_iter = iter(self.loaders['retain_train'])\n                    batch = next(retain_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 1: KMeans Neuron Clustering (KNC)\n# ============================================================================\n\nclass KMeansNeuronClustering(BaseUnlearner):\n    \"\"\"\n    NOVEL: Cluster neurons by activation patterns, mask forget-dominant clusters.\n    Theory: Neurons work in functional groups - cluster-level intervention.\n    \"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        \n        for i, layer in enumerate(layers):\n            # Collect activation patterns for this layer\n            forget_patterns = []\n            retain_patterns = []\n            \n            self.model.eval()\n            with torch.no_grad():\n                # Forget patterns\n                for batch in list(self.loaders['forget_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    acts = layer.ffn.lin1(h).mean(dim=(0, 1))  # Average over batch and sequence\n                    forget_patterns.append(acts.cpu().numpy())\n                \n                # Retain patterns\n                for batch in list(self.loaders['retain_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    acts = layer.ffn.lin1(h).mean(dim=(0, 1))\n                    retain_patterns.append(acts.cpu().numpy())\n            \n            if not forget_patterns or not retain_patterns:\n                continue\n            \n            # Stack: rows=samples, cols=neurons\n            forget_mat = np.stack(forget_patterns)  # (n_batches, n_neurons)\n            retain_mat = np.stack(retain_patterns)\n            \n            # Cluster neurons based on response patterns\n            n_neurons = forget_mat.shape[1]\n            # Transpose: rows=neurons, cols=samples\n            neuron_responses = np.concatenate([forget_mat.T, retain_mat.T], axis=1)  # (n_neurons, n_samples)\n            \n            n_clusters = min(8, n_neurons // 200)\n            if n_clusters < 2:\n                n_clusters = 2\n            \n            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(neuron_responses)\n            \n            # For each cluster, compute forget vs retain affinity\n            cluster_forget_score = np.zeros(n_clusters)\n            for c in range(n_clusters):\n                cluster_mask = cluster_labels == c\n                if cluster_mask.sum() > 0:\n                    # Average activation for neurons in this cluster\n                    f_mean = forget_mat[:, cluster_mask].mean()\n                    r_mean = retain_mat[:, cluster_mask].mean()\n                    cluster_forget_score[c] = f_mean - r_mean\n            \n            # Identify forget-dominant clusters\n            threshold = np.percentile(cluster_forget_score, 70)\n            forget_clusters = cluster_forget_score >= threshold\n            \n            # Create neuron-level mask\n            neuron_mask = torch.ones(n_neurons, dtype=torch.float32, device=self.device)\n            for c in range(n_clusters):\n                if forget_clusters[c]:\n                    cluster_neurons = cluster_labels == c\n                    neuron_mask[cluster_neurons] = 0.2  # Aggressive dampening\n            \n            # Apply mask\n            with torch.no_grad():\n                layer.ffn.lin1.weight.data *= neuron_mask.unsqueeze(1)\n                if layer.ffn.lin1.bias is not None:\n                    layer.ffn.lin1.bias.data *= neuron_mask\n        \n        # Repair\n        opt = torch.optim.AdamW(self.model.parameters(), lr=1.5e-5)\n        self.model.train()\n        \n        for epoch in range(4):\n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 2: PCA-Protected Gradient Ascent (PPGA)\n# ============================================================================\n\nclass PCAProtectedGradientAscent(BaseUnlearner):\n    \"\"\"\n    NOVEL: Use PCA to find retain principal components, project gradients away.\n    Theory: Retain knowledge lives in a low-dimensional subspace.\n    \"\"\"\n    def run(self):\n        # Collect retain activations from middle layer\n        layers = self.get_transformer_layers()\n        mid_layer = len(layers) // 2\n        \n        retain_activations = []\n        self.model.eval()\n        \n        with torch.no_grad():\n            for batch in list(self.loaders['retain_train'])[:15]:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                \n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                acts = out.hidden_states[mid_layer + 1][:, 0, :]  # CLS token\n                retain_activations.append(acts.cpu().numpy())\n        \n        retain_activations = np.vstack(retain_activations)  # (n_samples, 768)\n        \n        # PCA to find principal components\n        pca = PCA(n_components=16)  # Top 16 components\n        pca.fit(retain_activations)\n        principal_components = torch.tensor(pca.components_.T, dtype=torch.float32, device=self.device)  # (768, 16)\n        \n        # Gradient ascent with PCA projection\n        opt = torch.optim.AdamW(self.model.parameters(), lr=3e-5)\n        self.model.train()\n        \n        for epoch in range(5):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                # Project gradients away from retain PC space\n                # Only for FFN layers\n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if 'ffn' in name and 'weight' in name and param.grad is not None:\n                            if param.grad.shape[1] == 768:  # Input from hidden states\n                                # Project each row of gradient\n                                grad = param.grad.data  # (out_dim, 768)\n                                for pc_idx in range(principal_components.shape[1]):\n                                    pc = principal_components[:, pc_idx]  # (768,)\n                                    # Remove component in PC direction\n                                    projection = (grad @ pc.unsqueeze(1)) @ pc.unsqueeze(0)\n                                    grad -= 0.8 * projection\n                                param.grad.data = grad\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            # Repair\n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 3: Retain Anchor Regularization (RAR)\n# ============================================================================\n\nclass RetainAnchorRegularization(BaseUnlearner):\n    \"\"\"\n    NOVEL: Sample retain \"anchors\", regularize to stay close during forgetting.\n    Theory: Anchors prevent drift into regions that hurt retain performance.\n    \"\"\"\n    def run(self):\n        # Sample anchor embeddings from retain set\n        layers = self.get_transformer_layers()\n        n_layers = len(layers)\n        \n        anchors = {i: [] for i in range(n_layers)}\n        \n        self.model.eval()\n        with torch.no_grad():\n            for batch in list(self.loaders['retain_train'])[:8]:  # 8 batches = ~128 samples\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                \n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                \n                for layer_idx in range(n_layers):\n                    layer_hidden = out.hidden_states[layer_idx + 1][:, 0, :]  # CLS token\n                    anchors[layer_idx].append(layer_hidden.cpu())\n        \n        # Stack anchors\n        for layer_idx in range(n_layers):\n            if anchors[layer_idx]:\n                anchors[layer_idx] = torch.cat(anchors[layer_idx], dim=0).to(self.device)  # (n_anchors, 768)\n        \n        # Alternating training with anchor regularization\n        opt = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n        self.model.train()\n        \n        forget_iter = iter(self.loaders['forget_train'])\n        retain_iter = iter(self.loaders['retain_train'])\n        \n        n_steps = min(len(self.loaders['forget_train']), len(self.loaders['retain_train'])) * 5\n        \n        for step in range(n_steps):\n            if step % 2 == 0:\n                # Forget with anchor regularization\n                try:\n                    batch = next(forget_iter)\n                except StopIteration:\n                    forget_iter = iter(self.loaders['forget_train'])\n                    batch = next(forget_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                \n                # Forget loss\n                logits = self.model.classifier(out.last_hidden_state[:, 0, :])\n                forget_loss = F.cross_entropy(logits, labels)\n                \n                # Anchor regularization: minimize drift from anchors\n                anchor_loss = 0\n                for layer_idx in [len(layers) // 2, len(layers) * 2 // 3]:  # Middle and upper layers\n                    if layer_idx in anchors and len(anchors[layer_idx]) > 0:\n                        current_hidden = out.hidden_states[layer_idx + 1][:, 0, :]  # (batch, 768)\n                        anchor_samples = anchors[layer_idx][:min(32, len(anchors[layer_idx]))]  # Sample 32 anchors\n                        \n                        # Distance to nearest anchor\n                        distances = torch.cdist(current_hidden, anchor_samples)  # (batch, n_anchors)\n                        min_distances = distances.min(dim=1)[0]  # (batch,)\n                        anchor_loss += min_distances.mean()\n                \n                total_loss = -forget_loss + 0.3 * anchor_loss  # Forget but stay close to anchors\n                \n                total_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            else:\n                # Retain step\n                try:\n                    batch = next(retain_iter)\n                except StopIteration:\n                    retain_iter = iter(self.loaders['retain_train'])\n                    batch = next(retain_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 4: Critical Path Preservation (CPP)\n# ============================================================================\n\nclass CriticalPathPreservation(BaseUnlearner):\n    \"\"\"\n    NOVEL: Identify critical parameters for retain (high gradient magnitude), freeze them.\n    Theory: Some parameters are more important - protect them during forgetting.\n    \"\"\"\n    def run(self):\n        # Identify critical parameters for retain\n        param_importance = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                param_importance[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in param_importance and param.grad is not None:\n                    param_importance[name] += param.grad.data.abs()\n        \n        # Normalize importance\n        for name in param_importance:\n            param_importance[name] = param_importance[name] / (param_importance[name].max() + 1e-8)\n        \n        # Gradient ascent with parameter-wise learning rates\n        opt = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n        \n        for epoch in range(5):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                # Scale gradients inversely by importance (protect important params)\n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in param_importance and param.grad is not None:\n                            protection = param_importance[name]\n                            param.grad.data *= (1.0 - 0.8 * protection)  # High importance = less update\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            # Repair\n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 5: Adversarial Contrastive Unlearning (ACU)\n# ============================================================================\n\nclass AdversarialContrastiveUnlearning(BaseUnlearner):\n    \"\"\"\n    NOVEL: Push forget embeddings toward retain distribution via contrastive loss.\n    Theory: Make forget samples indistinguishable from retain in embedding space.\n    \"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        target_layer = len(layers) * 2 // 3\n        \n        # Collect retain embedding centers\n        retain_embeds = []\n        self.model.eval()\n        \n        with torch.no_grad():\n            for batch in list(self.loaders['retain_train'])[:12]:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                \n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                embeds = out.hidden_states[target_layer + 1][:, 0, :]\n                retain_embeds.append(embeds.cpu())\n        \n        retain_embeds = torch.cat(retain_embeds, dim=0).to(self.device)\n        retain_center = retain_embeds.mean(dim=0)\n        \n        # Training\n        opt = torch.optim.AdamW(self.model.parameters(), lr=3e-5)\n        self.model.train()\n        \n        for epoch in range(5):\n            # Push forget toward retain\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                \n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                forget_embeds = out.hidden_states[target_layer + 1][:, 0, :]\n                \n                # Contrastive: attract to retain center\n                attract_loss = F.mse_loss(forget_embeds, retain_center.unsqueeze(0).expand_as(forget_embeds))\n                \n                # Also do gradient ascent on classification\n                logits = self.model.classifier(out.last_hidden_state[:, 0, :])\n                labels = batch['labels'].to(self.device)\n                class_loss = F.cross_entropy(logits, labels)\n                \n                total_loss = -0.7 * class_loss + 0.3 * attract_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            # Repair\n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# NOVEL METHOD 6: Hierarchical Layer Unlearning (HLU)\n# ============================================================================\n\nclass HierarchicalLayerUnlearning(BaseUnlearner):\n    \"\"\"\n    NOVEL: Different forgetting rates per layer (aggressive in upper, gentle in lower).\n    Theory: Lower layers = general features, upper layers = class-specific.\n    \"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        n_layers = len(layers)\n        \n        # Define layer-wise forgetting strength (0=no forget, 1=max forget)\n        layer_strength = torch.linspace(0.3, 1.0, n_layers)  # Increase from bottom to top\n        \n        # Alternating training with layer-wise LR\n        forget_iter = iter(self.loaders['forget_train'])\n        retain_iter = iter(self.loaders['retain_train'])\n        \n        n_steps = min(len(self.loaders['forget_train']), len(self.loaders['retain_train'])) * 5\n        \n        self.model.train()\n        \n        for step in range(n_steps):\n            if step % 2 == 0:\n                # Forget step with layer-wise LR\n                try:\n                    batch = next(forget_iter)\n                except StopIteration:\n                    forget_iter = iter(self.loaders['forget_train'])\n                    batch = next(forget_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                # Layer-wise gradient scaling\n                with torch.no_grad():\n                    for layer_idx, layer in enumerate(layers):\n                        strength = layer_strength[layer_idx].item()\n                        for param in layer.parameters():\n                            if param.grad is not None:\n                                param.grad.data *= strength\n                \n                base_lr = 2e-5\n                with torch.no_grad():\n                    for param in self.model.parameters():\n                        if param.grad is not None:\n                            param.data += base_lr * param.grad\n            else:\n                # Retain step\n                try:\n                    batch = next(retain_iter)\n                except StopIteration:\n                    retain_iter = iter(self.loaders['retain_train'])\n                    batch = next(retain_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt = torch.optim.AdamW(self.model.parameters(), lr=1.5e-5)\n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# Data Loading and Experiment Runner\n# ============================================================================\n\ndef load_data(config, tokenizer):\n    raw = load_dataset(\"ag_news\")\n    data = defaultdict(lambda: defaultdict(list))\n    for split in ['train', 'test']:\n        for item in raw[split]:\n            data[split][item['label']].append(item['text'])\n    return data\n\n\ndef make_loader(texts, labels, tokenizer, batch_size, shuffle=False):\n    ds = TextClassificationDataset(texts, labels, tokenizer, 128)\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\n\ndef run_experiment():\n    conf = ExperimentConfig()\n    ReportStyler.print_banner(\"Transformer Unlearning v8.0 - SOTA Breakthrough 🎯\")\n    \n    print(f\"\\n{ReportStyler.BOLD}Configuration:{ReportStyler.ENDC}\")\n    print(f\"  Device: {conf.DEVICE}\")\n    print(f\"  Model: {conf.MODEL_NAME}\")\n    print(f\"  Seed: {conf.SEED}\")\n    print(f\"  Forget Scenarios: {conf.FORGET_SCENARIOS}\")\n    print(f\"\\n{ReportStyler.BOLD}Current SOTA:{ReportStyler.ENDC}\")\n    print(f\"  Best H-Score: {BEST_BASELINE_H_SCORE:.4f} ({BEST_BASELINE_METHOD})\")\n    print(f\"\\n{ReportStyler.BOLD}Method Portfolio (10 methods):{ReportStyler.ENDC}\")\n    print(f\"  PROVEN (3): GCM, HFGCM, ADU\")\n    print(f\"  NOVEL-CLUSTERING (1): KNC (KMeans neuron clustering)\")\n    print(f\"  NOVEL-SUBSPACE (2): PPGA (PCA protection), RAR (anchor regularization)\")\n    print(f\"  NOVEL-PATH (1): CPP (critical path preservation)\")\n    print(f\"  NOVEL-CONTRASTIVE (1): ACU (adversarial contrastive)\")\n    print(f\"  NOVEL-HIERARCHICAL (1): HLU (layer-wise forgetting)\")\n    \n    set_seed(conf.SEED)\n    \n    tokenizer = AutoTokenizer.from_pretrained(conf.MODEL_NAME)\n    raw_data = load_data(conf, tokenizer)\n    evaluator = Evaluator(conf.DEVICE, conf)\n\n    final_results = []\n    best_new_score = 0.0\n    best_new_method = None\n    best_new_scenario = None\n\n    for f_cls in range(conf.NUM_CLASSES):\n        class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n        target_name = class_names[f_cls]\n        \n        for n_forget in conf.FORGET_SCENARIOS:\n            ReportStyler.print_scenario_header(target_name, n_forget, conf.SEED)\n            \n            f_train_txt = raw_data['train'][f_cls][:n_forget]\n            f_train_lbl = [f_cls] * n_forget\n            \n            r_train_txt, r_train_lbl = [], []\n            for c in range(4):\n                if c != f_cls:\n                    r_train_txt.extend(raw_data['train'][c][:conf.RETAIN_SAMPLES_PER_CLASS])\n                    r_train_lbl.extend([c] * conf.RETAIN_SAMPLES_PER_CLASS)\n            \n            t_f_txt = raw_data['test'][f_cls][:200]\n            t_f_lbl = [f_cls] * 200\n            t_r_txt, t_r_lbl = [], []\n            for c in range(4):\n                if c != f_cls:\n                    t_r_txt.extend(raw_data['test'][c][:200])\n                    t_r_lbl.extend([c] * 200)\n\n            loaders = {\n                'forget_train': make_loader(f_train_txt, f_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                'retain_train': make_loader(r_train_txt, r_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                'forget_test': make_loader(t_f_txt, t_f_lbl, tokenizer, conf.BATCH_SIZE, False),\n                'retain_test': make_loader(t_r_txt, t_r_lbl, tokenizer, conf.BATCH_SIZE, False),\n                'test_all': make_loader(t_f_txt + t_r_txt, t_f_lbl + t_r_lbl, tokenizer, conf.BATCH_SIZE, False)\n            }\n\n            model = AutoModelForSequenceClassification.from_pretrained(conf.MODEL_NAME, num_labels=4).to(conf.DEVICE)\n            base_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n            ReportStyler.print_baseline_row(\"PRE-TRAINED\", base_metrics['f_acc'], base_metrics['r_acc'], base_metrics['all_acc'], base_metrics['ece'], base_metrics['mia_auc'])\n\n            model.train()\n            opt = torch.optim.AdamW(model.parameters(), lr=conf.LEARNING_RATE)\n            combined = ConcatDataset([loaders['forget_train'].dataset, loaders['retain_train'].dataset])\n            train_l = DataLoader(combined, batch_size=conf.BATCH_SIZE, shuffle=True)\n            \n            for _ in range(conf.FINETUNE_EPOCHS):\n                for batch in train_l:\n                    out = model(batch['input_ids'].to(conf.DEVICE), attention_mask=batch['attention_mask'].to(conf.DEVICE), labels=batch['labels'].to(conf.DEVICE))\n                    opt.zero_grad()\n                    out.loss.backward()\n                    opt.step()\n            \n            ft_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n            ReportStyler.print_baseline_row(\"FINE-TUNED\", ft_metrics['f_acc'], ft_metrics['r_acc'], ft_metrics['all_acc'], ft_metrics['ece'], ft_metrics['mia_auc'])\n            \n            if ft_metrics['f_acc'] < 0.7:\n                print(f\"{ReportStyler.FAIL}│ Skipping: Model failed to learn target class (F-Acc < 0.7).{ReportStyler.ENDC}\")\n                del model\n                clear_memory()\n                continue\n\n            ReportStyler.print_table_header()\n            \n            methods = [\n                # Proven champions\n                (\"GCM\", GradientConflictMinimization),\n                (\"HFGCM\", HybridFFNGradientConflict),\n                (\"ADU\", AlternatingDirectionUnlearning),\n                \n                # Novel methods\n                (\"KNC\", KMeansNeuronClustering),\n                (\"PPGA\", PCAProtectedGradientAscent),\n                (\"RAR\", RetainAnchorRegularization),\n                (\"CPP\", CriticalPathPreservation),\n                (\"ACU\", AdversarialContrastiveUnlearning),\n                (\"HLU\", HierarchicalLayerUnlearning),\n            ]\n\n            ft_model_cpu = safe_deepcopy_model(model).to('cpu')\n\n            for name, Cls in methods:\n                try:\n                    current_model = safe_deepcopy_model(ft_model_cpu).to(conf.DEVICE)\n                    \n                    t0 = time.time()\n                    unlearner = Cls(current_model, conf.DEVICE, loaders, conf, f_cls)\n                    unlearned_model = unlearner.run()\n                    dur = time.time() - t0\n                    \n                    res = evaluator.get_full_metrics(unlearned_model, loaders, f_cls)\n                    \n                    ReportStyler.print_result_row(\n                        name, res['f_acc'], res['r_acc'], res['all_acc'],\n                        res['ece'], res['mia_auc'], res['privacy_score'],\n                        res['h_score'], dur\n                    )\n                    \n                    scenario_str = f\"{target_name}/N={n_forget}/Seed={conf.SEED}\"\n                    \n                    if res['h_score'] > BEST_BASELINE_H_SCORE:\n                        ReportStyler.print_new_best(name, res['h_score'], scenario_str)\n                    \n                    if res['h_score'] > best_new_score:\n                        best_new_score = res['h_score']\n                        best_new_method = name\n                        best_new_scenario = scenario_str\n                    \n                    final_results.append({\n                        'class': target_name, 'n': n_forget, 'method': name,\n                        **res, 'time': dur\n                    })\n                    \n                    del unlearned_model, unlearner, current_model\n                    clear_memory()\n                    \n                except Exception as e:\n                    print(f\"│ {ReportStyler.FAIL}{name:<12} │ ERROR: {str(e)[:60]}{ReportStyler.ENDC}\")\n                    import traceback\n                    traceback.print_exc()\n\n            print(f\"│ {'─'*138}\")\n            del model, ft_model_cpu\n            clear_memory()\n\n    print(\"\\n\" + \"=\" * 140)\n    print(f\"{ReportStyler.BOLD}🏆 FINAL RESULTS - PUBLICATION READY 🏆{ReportStyler.ENDC}\")\n    print(\"=\" * 140)\n    \n    if final_results:\n        method_stats = defaultdict(lambda: {'f_acc': [], 'r_acc': [], 'h_score': [], 'privacy': [], 'time': []})\n        \n        for r in final_results:\n            m = r['method']\n            method_stats[m]['f_acc'].append(r['f_acc'])\n            method_stats[m]['r_acc'].append(r['r_acc'])\n            method_stats[m]['h_score'].append(r['h_score'])\n            method_stats[m]['privacy'].append(r['privacy_score'])\n            method_stats[m]['time'].append(r['time'])\n        \n        print(f\"\\n{'Method':<12} │ {'F-Acc':<16} │ {'R-Acc':<16} │ {'H-Score':<16} │ {'Privacy':<16} │ {'Time':<10}\")\n        print(\"-\" * 105)\n        \n        ranking = []\n        for method, stats in sorted(method_stats.items()):\n            f_mean, f_std = np.mean(stats['f_acc']), np.std(stats['f_acc'])\n            r_mean, r_std = np.mean(stats['r_acc']), np.std(stats['r_acc'])\n            h_mean, h_std = np.mean(stats['h_score']), np.std(stats['h_score'])\n            p_mean, p_std = np.mean(stats['privacy']), np.std(stats['privacy'])\n            t_mean = np.mean(stats['time'])\n            \n            beat_baseline = \"🏆\" if h_mean > BEST_BASELINE_H_SCORE else (\"=\" if abs(h_mean - BEST_BASELINE_H_SCORE) < 0.001 else \"\")\n            print(f\"{method:<12} │ {f_mean:.3f}±{f_std:.3f}      │ {r_mean:.3f}±{r_std:.3f}      │ {h_mean:.3f}±{h_std:.3f} {beat_baseline:<3} │ {p_mean:.3f}±{p_std:.3f}      │ {t_mean:.1f}s\")\n            ranking.append((method, h_mean, f_mean, r_mean, p_mean, t_mean))\n        \n        ranking.sort(key=lambda x: (-x[1], x[5]))\n        \n        print(\"\\n\" + \"=\" * 140)\n        print(f\"{ReportStyler.BOLD}🎯 FINAL RANKING 🎯{ReportStyler.ENDC}\")\n        print(\"=\" * 140)\n        for rank, (method, h, f, r, p, t) in enumerate(ranking, 1):\n            if h > BEST_BASELINE_H_SCORE:\n                indicator = f\" {ReportStyler.GREEN}★ NEW SOTA ★{ReportStyler.ENDC}\"\n            elif abs(h - BEST_BASELINE_H_SCORE) < 0.001:\n                indicator = f\" {ReportStyler.YELLOW}= TIED ={ReportStyler.ENDC}\"\n            else:\n                indicator = \"\"\n            print(f\"  {rank:2d}. {method:<12} H={h:.4f} F={f:.3f} R={r:.3f} P={p:.3f} T={t:.1f}s{indicator}\")\n        \n        print(\"\\n\" + \"=\" * 140)\n        print(f\"{ReportStyler.BOLD}📊 PAPER SUMMARY 📊{ReportStyler.ENDC}\")\n        print(\"=\" * 140)\n        \n        if best_new_score > BEST_BASELINE_H_SCORE:\n            print(f\"{ReportStyler.BOLD}{ReportStyler.GREEN}✨ NEW STATE-OF-THE-ART ACHIEVED! ✨{ReportStyler.ENDC}\")\n            print(f\"   Method: {best_new_method}\")\n            print(f\"   H-Score: {best_new_score:.4f}\")\n            print(f\"   Improvement: +{(best_new_score - BEST_BASELINE_H_SCORE)*100:.2f}%\")\n            print(f\"   Scenario: {best_new_scenario}\")\n        else:\n            print(f\"{ReportStyler.YELLOW}Best Method: {best_new_method} (H={best_new_score:.4f}){ReportStyler.ENDC}\")\n            if abs(best_new_score - BEST_BASELINE_H_SCORE) < 0.001:\n                print(f\"   Status: TIED with baseline\")\n            else:\n                print(f\"   Gap: {(BEST_BASELINE_H_SCORE - best_new_score)*100:.2f}%\")\n        \n        print(\"\\n\" + \"=\" * 140)\n        print(f\"{ReportStyler.BOLD}📝 THEORETICAL CONTRIBUTIONS:{ReportStyler.ENDC}\")\n        print(\"=\" * 140)\n        print(\"  1. GCM: Gradient conflict detection separates shared vs class-specific parameters\")\n        print(\"  2. KNC: Functional neuron clustering enables cluster-level intervention\")\n        print(\"  3. PPGA: PCA subspace protection preserves retain knowledge geometry\")\n        print(\"  4. RAR: Anchor regularization prevents drift in representation space\")\n        print(\"  5. CPP: Critical path analysis identifies and protects important parameters\")\n        print(\"  6. ACU: Adversarial contrastive learning confuses class boundaries\")\n        print(\"  7. HLU: Hierarchical layer treatment exploits abstraction levels\")\n        print(\"=\" * 140)\n\n    return final_results\n\n\nif __name__ == \"__main__\":\n    results = run_experiment()","metadata":{"execution":{"iopub.status.busy":"2025-12-29T23:52:41.718851Z","iopub.execute_input":"2025-12-29T23:52:41.719684Z","iopub.status.idle":"2025-12-30T00:15:53.608653Z","shell.execute_reply.started":"2025-12-29T23:52:41.719652Z","shell.execute_reply":"2025-12-30T00:15:53.607863Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\u001b[1m============================================================================================================================================\u001b[0m\n\u001b[1m\u001b[95m  TRANSFORMER UNLEARNING V8.0 - SOTA BREAKTHROUGH 🎯\u001b[0m\n\u001b[1m============================================================================================================================================\u001b[0m\n\n\u001b[1mConfiguration:\u001b[0m\n  Device: cuda\n  Model: distilbert-base-uncased\n  Seed: 42\n  Forget Scenarios: [50, 100, 200]\n\n\u001b[1mCurrent SOTA:\u001b[0m\n  Best H-Score: 0.9390 (NPO)\n\n\u001b[1mMethod Portfolio (10 methods):\u001b[0m\n  PROVEN (3): GCM, HFGCM, ADU\n  NOVEL-CLUSTERING (1): KNC (KMeans neuron clustering)\n  NOVEL-SUBSPACE (2): PPGA (PCA protection), RAR (anchor regularization)\n  NOVEL-PATH (1): CPP (critical path preservation)\n  NOVEL-CONTRASTIVE (1): ACU (adversarial contrastive)\n  NOVEL-HIERARCHICAL (1): HLU (layer-wise forgetting)\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.308 │ All-Acc: 0.231 │ ECE: 0.034 │ MIA: 0.378\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.615 │ R-Acc: 0.868 │ All-Acc: 0.805 │ ECE: 0.065 │ MIA: 0.196\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.333 │ All-Acc: 0.251 │ ECE: 0.110 │ MIA: 0.664\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.790 │ R-Acc: 0.858 │ All-Acc: 0.841 │ ECE: 0.026 │ MIA: 0.342\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD       │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM          │ \u001b[92m0.000   \u001b[0m │ 0.868   \u001b[0m │ 0.651    │ 0.254    │ 0.004    │ 0.008    │ 0.930   \u001b[0m │ 29.2   \n│ HFGCM        │ \u001b[92m0.000   \u001b[0m │ 0.838   \u001b[0m │ 0.629    │ 0.275    │ 0.009    │ 0.018    │ 0.912   \u001b[0m │ 26.6   \n│ ADU          │ \u001b[92m0.000   \u001b[0m │ 0.858   \u001b[0m │ 0.644    │ 0.265    │ 0.000    │ 0.001    │ 0.924   \u001b[0m │ 5.1    \n│ KNC          │ 0.525   \u001b[0m │ 0.878   \u001b[0m │ 0.790    │ 0.091    │ 0.115    │ 0.231    │ 0.617   \u001b[0m │ 23.6   \n│ PPGA         │ \u001b[92m0.000   \u001b[0m │ 0.857   \u001b[0m │ 0.642    │ 0.262    │ 0.007    │ 0.014    │ 0.923   \u001b[0m │ 29.1   \n│ RAR          │ 0.760   \u001b[0m │ 0.833   \u001b[0m │ 0.815    │ 0.124    │ 0.215    │ 0.431    │ 0.373   \u001b[0m │ 5.3    \n│ CPP          │ \u001b[92m0.000   \u001b[0m │ 0.887   \u001b[0m │ 0.665    │ 0.262    │ 0.009    │ 0.019    │ \u001b[1m\u001b[92m0.940   \u001b[0m │ 29.0   \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'CPP' H-Score=0.9399\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +0.09%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: World/N=100/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ ACU          │ 0.450   \u001b[0m │ 0.865   \u001b[0m │ 0.761    │ 0.143    │ 0.102    │ 0.204    │ 0.672   \u001b[0m │ 28.4   \n│ HLU          │ 0.710   \u001b[0m │ 0.873   \u001b[0m │ 0.833    │ 0.066    │ 0.206    │ 0.412    │ 0.435   \u001b[0m │ 4.9    \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.060 │ MIA: 0.520\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.825 │ R-Acc: 0.803 │ All-Acc: 0.809 │ ECE: 0.065 │ MIA: 0.732\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD       │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM          │ \u001b[92m0.000   \u001b[0m │ 0.865   \u001b[0m │ 0.649    │ 0.255    │ 0.001    │ 0.002    │ 0.928   \u001b[0m │ 32.3   \n│ HFGCM        │ \u001b[92m0.000   \u001b[0m │ 0.875   \u001b[0m │ 0.656    │ 0.248    │ 0.000    │ 0.001    │ 0.933   \u001b[0m │ 29.8   \n│ ADU          │ \u001b[92m0.000   \u001b[0m │ 0.840   \u001b[0m │ 0.630    │ 0.304    │ 0.000    │ 0.000    │ 0.913   \u001b[0m │ 8.5    \n│ KNC          │ 0.465   \u001b[0m │ 0.863   \u001b[0m │ 0.764    │ 0.103    │ 0.111    │ 0.222    │ 0.661   \u001b[0m │ 24.4   \n│ PPGA         │ \u001b[92m0.000   \u001b[0m │ 0.878   \u001b[0m │ 0.659    │ 0.230    │ 0.000    │ 0.001    │ 0.935   \u001b[0m │ 32.6   \n│ RAR          │ 0.765   \u001b[0m │ 0.813   \u001b[0m │ 0.801    │ 0.119    │ 0.230    │ 0.461    │ 0.365   \u001b[0m │ 8.7    \n│ CPP          │ \u001b[92m0.000   \u001b[0m │ 0.860   \u001b[0m │ 0.645    │ 0.271    │ 0.001    │ 0.001    │ 0.925   \u001b[0m │ 32.3   \n│ ACU          │ \u001b[92m0.000   \u001b[0m │ 0.872   \u001b[0m │ 0.654    │ 0.148    │ 0.067    │ 0.134    │ 0.931   \u001b[0m │ 31.5   \n│ HLU          │ 0.800   \u001b[0m │ 0.820   \u001b[0m │ 0.815    │ 0.096    │ 0.426    │ 0.852    │ 0.322   \u001b[0m │ 8.2    \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.285 │ All-Acc: 0.214 │ ECE: 0.052 │ MIA: 0.027\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.460 │ R-Acc: 0.793 │ All-Acc: 0.710 │ ECE: 0.100 │ MIA: 0.310\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.315 │ All-Acc: 0.236 │ ECE: 0.051 │ MIA: 0.322\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.890 │ R-Acc: 0.785 │ All-Acc: 0.811 │ ECE: 0.046 │ MIA: 0.515\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD       │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM          │ \u001b[92m0.000   \u001b[0m │ 0.797   \u001b[0m │ 0.598    │ 0.312    │ 0.028    │ 0.055    │ 0.887   \u001b[0m │ 29.1   \n│ HFGCM        │ \u001b[92m0.000   \u001b[0m │ 0.810   \u001b[0m │ 0.608    │ 0.290    │ 0.011    │ 0.022    │ 0.895   \u001b[0m │ 26.6   \n│ ADU          │ \u001b[92m0.000   \u001b[0m │ 0.805   \u001b[0m │ 0.604    │ 0.305    │ 0.000    │ 0.000    │ 0.892   \u001b[0m │ 5.1    \n│ KNC          │ 0.115   \u001b[0m │ 0.795   \u001b[0m │ 0.625    │ 0.240    │ 0.175    │ 0.350    │ 0.838   \u001b[0m │ 23.6   \n│ PPGA         │ \u001b[92m0.000   \u001b[0m │ 0.782   \u001b[0m │ 0.586    │ 0.312    │ 0.003    │ 0.006    │ 0.877   \u001b[0m │ 29.1   \n│ RAR          │ 0.855   \u001b[0m │ 0.800   \u001b[0m │ 0.814    │ 0.085    │ 0.247    │ 0.493    │ 0.246   \u001b[0m │ 5.2    \n│ CPP          │ \u001b[92m0.000   \u001b[0m │ 0.802   \u001b[0m │ 0.601    │ 0.307    │ 0.003    │ 0.005    │ 0.890   \u001b[0m │ 29.1   \n│ ACU          │ \u001b[92m0.000   \u001b[0m │ 0.805   \u001b[0m │ 0.604    │ 0.229    │ 0.157    │ 0.314    │ 0.892   \u001b[0m │ 28.4   \n│ HLU          │ 0.765   \u001b[0m │ 0.790   \u001b[0m │ 0.784    │ 0.068    │ 0.375    │ 0.749    │ 0.362   \u001b[0m │ 4.9    \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.695 │ R-Acc: 0.087 │ All-Acc: 0.239 │ ECE: 0.024 │ MIA: 0.951\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.920 │ R-Acc: 0.763 │ All-Acc: 0.802 │ ECE: 0.082 │ MIA: 0.973\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD       │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM          │ \u001b[92m0.000   \u001b[0m │ 0.810   \u001b[0m │ 0.608    │ 0.302    │ 0.001    │ 0.003    │ 0.895   \u001b[0m │ 32.3   \n│ HFGCM        │ \u001b[92m0.000   \u001b[0m │ 0.783   \u001b[0m │ 0.588    │ 0.311    │ 0.000    │ 0.001    │ 0.879   \u001b[0m │ 29.8   \n│ ADU          │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.305    │ 0.000    │ 0.000    │ 0.893   \u001b[0m │ 8.5    \n│ KNC          │ 0.605   \u001b[0m │ 0.812   \u001b[0m │ 0.760    │ 0.078    │ 0.248    │ 0.496    │ 0.531   \u001b[0m │ 24.3   \n│ PPGA         │ \u001b[92m0.000   \u001b[0m │ 0.800   \u001b[0m │ 0.600    │ 0.299    │ 0.000    │ 0.000    │ 0.889   \u001b[0m │ 32.6   \n│ RAR          │ 0.790   \u001b[0m │ 0.808   \u001b[0m │ 0.804    │ 0.156    │ 0.205    │ 0.410    │ 0.333   \u001b[0m │ 8.7    \n│ CPP          │ \u001b[92m0.000   \u001b[0m │ 0.808   \u001b[0m │ 0.606    │ 0.294    │ 0.000    │ 0.000    │ 0.894   \u001b[0m │ 32.3   \n│ ACU          │ \u001b[92m0.005   \u001b[0m │ 0.820   \u001b[0m │ 0.616    │ 0.173    │ 0.145    │ 0.291    │ 0.899   \u001b[0m │ 31.5   \n│ HLU          │ 0.855   \u001b[0m │ 0.798   \u001b[0m │ 0.812    │ 0.083    │ 0.507    │ 0.987    │ 0.245   \u001b[0m │ 8.2    \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.423 │ All-Acc: 0.318 │ ECE: 0.055 │ MIA: 0.009\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.670 │ R-Acc: 0.863 │ All-Acc: 0.815 │ ECE: 0.033 │ MIA: 0.188\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.565 │ R-Acc: 0.265 │ All-Acc: 0.340 │ ECE: 0.067 │ MIA: 0.713\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.685 │ R-Acc: 0.828 │ All-Acc: 0.792 │ ECE: 0.070 │ MIA: 0.292\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.120 │ R-Acc: 0.252 │ All-Acc: 0.219 │ ECE: 0.049 │ MIA: 0.666\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.900 │ R-Acc: 0.808 │ All-Acc: 0.831 │ ECE: 0.044 │ MIA: 0.459\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD       │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM          │ \u001b[92m0.000   \u001b[0m │ 0.918   \u001b[0m │ 0.689    │ 0.242    │ 0.017    │ 0.034    │ \u001b[1m\u001b[92m0.957   \u001b[0m │ 32.3   \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'GCM' H-Score=0.9574\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +1.84%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: Business/N=200/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ HFGCM        │ \u001b[92m0.000   \u001b[0m │ 0.908   \u001b[0m │ 0.681    │ 0.245    │ 0.030    │ 0.059    │ \u001b[1m\u001b[92m0.952   \u001b[0m │ 29.8   \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'HFGCM' H-Score=0.9520\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +1.30%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: Business/N=200/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ ADU          │ \u001b[92m0.000   \u001b[0m │ 0.913   \u001b[0m │ 0.685    │ 0.277    │ 0.000    │ 0.000    │ \u001b[1m\u001b[92m0.955   \u001b[0m │ 8.5    \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'ADU' H-Score=0.9547\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +1.57%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: Business/N=200/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ KNC          │ 0.525   \u001b[0m │ 0.890   \u001b[0m │ 0.799    │ 0.071    │ 0.124    │ 0.247    │ 0.619   \u001b[0m │ 24.3   \n│ PPGA         │ \u001b[92m0.000   \u001b[0m │ 0.917   \u001b[0m │ 0.688    │ 0.252    │ 0.017    │ 0.035    │ \u001b[1m\u001b[92m0.957   \u001b[0m │ 32.4   \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'PPGA' H-Score=0.9565\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +1.75%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: Business/N=200/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ RAR          │ 0.550   \u001b[0m │ 0.903   \u001b[0m │ 0.815    │ 0.069    │ 0.099    │ 0.198    │ 0.601   \u001b[0m │ 8.7    \n│ CPP          │ \u001b[92m0.000   \u001b[0m │ 0.912   \u001b[0m │ 0.684    │ 0.260    │ 0.023    │ 0.045    │ \u001b[1m\u001b[92m0.954   \u001b[0m │ 32.2   \n\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\u001b[1m\u001b[92m  🏆 NEW SOTA! 'CPP' H-Score=0.9538\u001b[0m\n\u001b[1m\u001b[92m  Beats NPO (0.9390) by +1.48%\u001b[0m\n\u001b[1m\u001b[92m  Scenario: Business/N=200/Seed=42\u001b[0m\n\u001b[1m\u001b[92m★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\u001b[0m\n\n│ ACU          │ 0.375   \u001b[0m │ 0.910   \u001b[0m │ 0.776    │ 0.073    │ 0.081    │ 0.162    │ 0.741   \u001b[0m │ 31.5   \n│ HLU          │ 0.715   \u001b[0m │ 0.865   \u001b[0m │ 0.828    │ 0.067    │ 0.244    │ 0.489    │ 0.429   \u001b[0m │ 8.2    \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.525 │ R-Acc: 0.112 │ All-Acc: 0.215 │ ECE: 0.059 │ MIA: 0.880\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.230 │ R-Acc: 0.887 │ All-Acc: 0.723 │ ECE: 0.129 │ MIA: 0.193\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.545 │ R-Acc: 0.135 │ All-Acc: 0.237 │ ECE: 0.029 │ MIA: 0.890\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.515 │ R-Acc: 0.880 │ All-Acc: 0.789 │ ECE: 0.077 │ MIA: 0.258\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.267 │ All-Acc: 0.201 │ ECE: 0.070 │ MIA: 0.302\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.485 │ R-Acc: 0.893 │ All-Acc: 0.791 │ ECE: 0.079 │ MIA: 0.379\n\u001b[91m│ Skipping: Model failed to learn target class (F-Acc < 0.7).\u001b[0m\n\n============================================================================================================================================\n\u001b[1m🏆 FINAL RESULTS - PUBLICATION READY 🏆\u001b[0m\n============================================================================================================================================\n\nMethod       │ F-Acc            │ R-Acc            │ H-Score          │ Privacy          │ Time      \n---------------------------------------------------------------------------------------------------------\nACU          │ 0.166±0.203      │ 0.854±0.038      │ 0.827±0.102     │ 0.221±0.071      │ 30.3s\nADU          │ 0.000±0.000      │ 0.845±0.040      │ 0.915±0.023     │ 0.000±0.000      │ 7.1s\nCPP          │ 0.000±0.000      │ 0.854±0.043      │ 0.920±0.025     │ 0.014±0.017      │ 31.0s\nGCM          │ 0.000±0.000      │ 0.852±0.044      │ 0.919±0.026     │ 0.020±0.021      │ 31.0s\nHFGCM        │ 0.000±0.000      │ 0.843±0.045      │ 0.914±0.026     │ 0.020±0.021      │ 28.5s\nHLU          │ 0.769±0.054      │ 0.829±0.034      │ 0.359±0.071     │ 0.698±0.217      │ 6.9s\nKNC          │ 0.447±0.172      │ 0.848±0.038      │ 0.653±0.101     │ 0.309±0.104      │ 24.0s\nPPGA         │ 0.000±0.000      │ 0.847±0.050      │ 0.916±0.029     │ 0.011±0.013      │ 31.2s\nRAR          │ 0.744±0.103      │ 0.832±0.037      │ 0.383±0.118     │ 0.398±0.104      │ 7.3s\n\n============================================================================================================================================\n\u001b[1m🎯 FINAL RANKING 🎯\u001b[0m\n============================================================================================================================================\n   1. CPP          H=0.9205 F=0.000 R=0.854 P=0.014 T=31.0s\n   2. GCM          H=0.9193 F=0.000 R=0.852 P=0.020 T=31.0s\n   3. PPGA         H=0.9162 F=0.000 R=0.847 P=0.011 T=31.2s\n   4. ADU          H=0.9153 F=0.000 R=0.845 P=0.000 T=7.1s\n   5. HFGCM        H=0.9142 F=0.000 R=0.843 P=0.020 T=28.5s\n   6. ACU          H=0.8272 F=0.166 R=0.854 P=0.221 T=30.3s\n   7. KNC          H=0.6531 F=0.447 R=0.848 P=0.309 T=24.0s\n   8. RAR          H=0.3834 F=0.744 R=0.832 P=0.398 T=7.3s\n   9. HLU          H=0.3587 F=0.769 R=0.829 P=0.698 T=6.9s\n\n============================================================================================================================================\n\u001b[1m📊 PAPER SUMMARY 📊\u001b[0m\n============================================================================================================================================\n\u001b[1m\u001b[92m✨ NEW STATE-OF-THE-ART ACHIEVED! ✨\u001b[0m\n   Method: GCM\n   H-Score: 0.9574\n   Improvement: +1.84%\n   Scenario: Business/N=200/Seed=42\n\n============================================================================================================================================\n\u001b[1m📝 THEORETICAL CONTRIBUTIONS:\u001b[0m\n============================================================================================================================================\n  1. GCM: Gradient conflict detection separates shared vs class-specific parameters\n  2. KNC: Functional neuron clustering enables cluster-level intervention\n  3. PPGA: PCA subspace protection preserves retain knowledge geometry\n  4. RAR: Anchor regularization prevents drift in representation space\n  5. CPP: Critical path analysis identifies and protects important parameters\n  6. ACU: Adversarial contrastive learning confuses class boundaries\n  7. HLU: Hierarchical layer treatment exploits abstraction levels\n============================================================================================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# top method deep testing ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\nfrom datasets import load_dataset\nimport numpy as np\nimport copy\nimport time\nfrom collections import defaultdict\nimport warnings\nimport gc\nfrom typing import Dict, List\nfrom dataclasses import dataclass\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings('ignore')\nlogging.set_verbosity_error()\n\n@dataclass\nclass ExperimentConfig:\n    MODEL_NAME: str = \"distilbert-base-uncased\"\n    DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    BATCH_SIZE: int = 16\n    MAX_LENGTH: int = 128\n    LEARNING_RATE: float = 2e-5\n    FINETUNE_EPOCHS: int = 3\n    RETAIN_SAMPLES_PER_CLASS: int = 200\n    FORGET_SCENARIOS: List[int] = None\n    SEEDS: List[int] = None\n    NUM_CLASSES: int = 4\n    \n    def __post_init__(self):\n        if self.SEEDS is None:\n            self.SEEDS = [42, 123, 456]\n        if self.FORGET_SCENARIOS is None:\n            self.FORGET_SCENARIOS = [50, 100, 200]\n\n\nclass ReportStyler:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    CYAN = '\\033[96m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n\n    @staticmethod\n    def print_banner(text):\n        print(f\"\\n{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{ReportStyler.HEADER}  {text.upper()}{ReportStyler.ENDC}\")\n        print(f\"{ReportStyler.BOLD}{'='*140}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_scenario_header(cls_name, n, seed):\n        print(f\"\\n{ReportStyler.CYAN}┌── SCENARIO: Forget '{cls_name}' | N={n} | Seed={seed} {'─'*80}{ReportStyler.ENDC}\")\n\n    @staticmethod\n    def print_baseline_row(name, f_acc, r_acc, all_acc, ece, mia_auc):\n        print(f\"│ {ReportStyler.YELLOW}{name:<12}{ReportStyler.ENDC} │ F-Acc: {f_acc:.3f} │ R-Acc: {r_acc:.3f} │ All-Acc: {all_acc:.3f} │ ECE: {ece:.3f} │ MIA: {mia_auc:.3f}\")\n\n    @staticmethod\n    def print_table_header():\n        print(f\"│ {'─'*138}\")\n        print(f\"│ {ReportStyler.BOLD}{'METHOD':<10} │ {'F-ACC':<8} │ {'R-ACC':<8} │ {'ALL-ACC':<8} │ {'ECE':<8} │ {'MIA-AUC':<8} │ {'PRIVACY':<8} │ {'H-SCORE':<8} │ {'TIME':<7}{ReportStyler.ENDC}\")\n        print(f\"│ {'─'*138}\")\n\n    @staticmethod\n    def print_result_row(method, f_acc, r_acc, all_acc, ece, mia, priv, h_score, elapsed):\n        c_f = ReportStyler.GREEN if f_acc < 0.05 else \"\"\n        c_r = ReportStyler.FAIL if r_acc < 0.7 else \"\"\n        c_h = ReportStyler.BOLD if h_score > 0.85 else \"\"\n        \n        print(f\"│ {method:<10} │ {c_f}{f_acc:<8.3f}{ReportStyler.ENDC} │ {c_r}{r_acc:<8.3f}{ReportStyler.ENDC} │ {all_acc:<8.3f} │ {ece:<8.3f} │ {mia:<8.3f} │ {priv:<8.3f} │ {c_h}{h_score:<8.3f}{ReportStyler.ENDC} │ {elapsed:<7.1f}\")\n\n\ndef set_seed(seed: int):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\ndef safe_deepcopy_model(model):\n    clear_memory()\n    device = next(model.parameters()).device\n    model_cpu = model.to('cpu')\n    model_copy = copy.deepcopy(model_cpu)\n    model.to(device)\n    return model_copy\n\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n\nclass Evaluator:\n    def __init__(self, device, config):\n        self.device = device\n        self.config = config\n        \n    def get_full_metrics(self, model, loaders, forget_class):\n        model.eval()\n        model.to(self.device)\n        \n        all_preds, all_labels, all_probs = [], [], []\n        with torch.no_grad():\n            for batch in loaders['test_all']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                probs = F.softmax(out.logits, dim=-1)\n                all_preds.extend(torch.argmax(probs, dim=-1).cpu().numpy())\n                all_labels.extend(batch['labels'].cpu().numpy())\n                all_probs.extend(probs.cpu().numpy())\n        \n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        all_probs = np.array(all_probs)\n        \n        acc_per_class = {}\n        for c in range(self.config.NUM_CLASSES):\n            mask = all_labels == c\n            acc_per_class[c] = accuracy_score(all_labels[mask], all_preds[mask]) if mask.sum() > 0 else 0.0\n\n        confidences = np.max(all_probs, axis=1)\n        predictions = np.argmax(all_probs, axis=1)\n        accuracies = (predictions == all_labels)\n        bins = np.linspace(0, 1, 11)\n        ece = 0.0\n        for i in range(10):\n            mask = (confidences >= bins[i]) & (confidences < bins[i+1])\n            if mask.sum() > 0:\n                ece += mask.sum() / len(all_labels) * abs(accuracies[mask].mean() - confidences[mask].mean())\n\n        crit = nn.CrossEntropyLoss(reduction='none')\n        f_losses, r_losses = [], []\n        with torch.no_grad():\n            for batch in loaders['forget_train']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                f_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n            for batch in loaders['retain_test']:\n                out = model(batch['input_ids'].to(self.device), attention_mask=batch['attention_mask'].to(self.device))\n                r_losses.extend(crit(out.logits, batch['labels'].to(self.device)).cpu().numpy())\n        \n        y_true = np.concatenate([np.ones(len(f_losses)), np.zeros(len(r_losses))])\n        y_score = np.concatenate([-np.array(f_losses), -np.array(r_losses)])\n        try:\n            mia_auc = roc_auc_score(y_true, y_score)\n        except:\n            mia_auc = 0.5\n        \n        privacy_score = 1.0 - (2 * abs(mia_auc - 0.5))\n\n        f_acc = acc_per_class[forget_class]\n        r_acc = np.mean([acc_per_class[c] for c in range(4) if c != forget_class])\n        \n        eff = 1.0 - f_acc\n        pres = r_acc\n        h_score = 2 * (eff * pres) / (eff + pres + 1e-6)\n\n        return {\n            'f_acc': f_acc,\n            'r_acc': r_acc,\n            'all_acc': accuracy_score(all_labels, all_preds),\n            'ece': ece,\n            'mia_auc': mia_auc,\n            'privacy_score': privacy_score,\n            'h_score': h_score\n        }\n\n\nclass BaseUnlearner:\n    def __init__(self, model, device, loaders, config, forget_class):\n        self.model = safe_deepcopy_model(model).to(device)\n        self.device = device\n        self.loaders = loaders\n        self.config = config\n        self.forget_class = forget_class\n        self.ref_model = safe_deepcopy_model(model).to(device)\n        self.ref_model.eval()\n        for p in self.ref_model.parameters():\n            p.requires_grad = False\n    \n    def get_transformer_layers(self):\n        return list(self.model.distilbert.transformer.layer)\n    \n    def _kl_loss(self, logits1, logits2):\n        return F.kl_div(\n            F.log_softmax(logits1, dim=-1),\n            F.softmax(logits2, dim=-1),\n            reduction='batchmean'\n        )\n\n\n# ============================================================================\n# TOP 5 SOTA METHODS\n# ============================================================================\n\nclass GradientConflictMinimization(BaseUnlearner):\n    \"\"\"#1 - H-Score: 0.9574\"\"\"\n    def run(self):\n        retain_grad_signature = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                retain_grad_signature[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in retain_grad_signature and param.grad is not None:\n                    retain_grad_signature[name] += torch.sign(param.grad.data)\n        \n        for name in retain_grad_signature:\n            retain_grad_signature[name] = torch.sign(retain_grad_signature[name])\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=3e-5)\n        \n        for epoch in range(5):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in retain_grad_signature and param.grad is not None:\n                            forget_grad_sign = torch.sign(param.grad.data)\n                            conflict_mask = (forget_grad_sign == retain_grad_signature[name]).float()\n                            param.grad.data *= conflict_mask\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\nclass PCAProtectedGradientAscent(BaseUnlearner):\n    \"\"\"#2 - H-Score: 0.9565\"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        mid_layer = len(layers) // 2\n        \n        retain_activations = []\n        self.model.eval()\n        \n        with torch.no_grad():\n            for batch in list(self.loaders['retain_train'])[:15]:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                \n                out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                acts = out.hidden_states[mid_layer + 1][:, 0, :]\n                retain_activations.append(acts.cpu().numpy())\n        \n        retain_activations = np.vstack(retain_activations)\n        \n        pca = PCA(n_components=16)\n        pca.fit(retain_activations)\n        principal_components = torch.tensor(pca.components_.T, dtype=torch.float32, device=self.device)\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=3e-5)\n        self.model.train()\n        \n        for epoch in range(5):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if 'ffn' in name and 'weight' in name and param.grad is not None:\n                            if param.grad.shape[1] == 768:\n                                grad = param.grad.data\n                                for pc_idx in range(principal_components.shape[1]):\n                                    pc = principal_components[:, pc_idx]\n                                    projection = (grad @ pc.unsqueeze(1)) @ pc.unsqueeze(0)\n                                    grad -= 0.8 * projection\n                                param.grad.data = grad\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\nclass AlternatingDirectionUnlearning(BaseUnlearner):\n    \"\"\"#3 - H-Score: 0.9547\"\"\"\n    def run(self):\n        opt = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n        self.model.train()\n        \n        forget_iter = iter(self.loaders['forget_train'])\n        retain_iter = iter(self.loaders['retain_train'])\n        \n        n_steps = min(len(self.loaders['forget_train']), len(self.loaders['retain_train'])) * 5\n        \n        for step in range(n_steps):\n            if step % 2 == 0:\n                try:\n                    batch = next(forget_iter)\n                except StopIteration:\n                    forget_iter = iter(self.loaders['forget_train'])\n                    batch = next(forget_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            else:\n                try:\n                    batch = next(retain_iter)\n                except StopIteration:\n                    retain_iter = iter(self.loaders['retain_train'])\n                    batch = next(retain_iter)\n                \n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.8 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n        \n        return self.model\n\n\nclass CriticalPathPreservation(BaseUnlearner):\n    \"\"\"#4 - H-Score: 0.9538\"\"\"\n    def run(self):\n        param_importance = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                param_importance[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in param_importance and param.grad is not None:\n                    param_importance[name] += param.grad.data.abs()\n        \n        for name in param_importance:\n            param_importance[name] = param_importance[name] / (param_importance[name].max() + 1e-8)\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n        \n        for epoch in range(5):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in param_importance and param.grad is not None:\n                            protection = param_importance[name]\n                            param.grad.data *= (1.0 - 0.8 * protection)\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\nclass HybridFFNGradientConflict(BaseUnlearner):\n    \"\"\"#5 - H-Score: 0.9520\"\"\"\n    def run(self):\n        layers = self.get_transformer_layers()\n        \n        forget_acts = {i: [] for i in range(len(layers))}\n        retain_acts = {i: [] for i in range(len(layers))}\n        \n        self.model.eval()\n        \n        for i, layer in enumerate(layers):\n            with torch.no_grad():\n                for batch in list(self.loaders['forget_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    ffn_out = layer.ffn.lin1(h)\n                    forget_acts[i].append(ffn_out.mean(dim=(0, 1)).cpu())\n                \n                for batch in list(self.loaders['retain_train'])[:10]:\n                    ids = batch['input_ids'].to(self.device)\n                    mask = batch['attention_mask'].to(self.device)\n                    \n                    out = self.model.distilbert(ids, attention_mask=mask, output_hidden_states=True)\n                    h = out.hidden_states[i + 1]\n                    ffn_out = layer.ffn.lin1(h)\n                    retain_acts[i].append(ffn_out.mean(dim=(0, 1)).cpu())\n        \n        with torch.no_grad():\n            for i, layer in enumerate(layers):\n                if forget_acts[i] and retain_acts[i]:\n                    f_act = torch.stack(forget_acts[i]).mean(dim=0).to(self.device)\n                    r_act = torch.stack(retain_acts[i]).mean(dim=0).to(self.device)\n                    \n                    diff = (f_act - r_act)\n                    threshold = diff.quantile(0.75)\n                    prune_mask = (diff < threshold).float()\n                    \n                    layer.ffn.lin1.weight.data *= prune_mask.unsqueeze(1)\n                    if layer.ffn.lin1.bias is not None:\n                        layer.ffn.lin1.bias.data *= prune_mask\n        \n        retain_grad_signature = {}\n        for name, param in self.model.named_parameters():\n            if 'transformer.layer' in name:\n                retain_grad_signature[name] = torch.zeros_like(param)\n        \n        self.model.train()\n        for batch in list(self.loaders['retain_train'])[:10]:\n            ids = batch['input_ids'].to(self.device)\n            mask = batch['attention_mask'].to(self.device)\n            labels = batch['labels'].to(self.device)\n            \n            self.model.zero_grad()\n            out = self.model(ids, attention_mask=mask, labels=labels)\n            out.loss.backward()\n            \n            for name, param in self.model.named_parameters():\n                if name in retain_grad_signature and param.grad is not None:\n                    retain_grad_signature[name] += torch.sign(param.grad.data)\n        \n        for name in retain_grad_signature:\n            retain_grad_signature[name] = torch.sign(retain_grad_signature[name])\n        \n        opt = torch.optim.AdamW(self.model.parameters(), lr=2.5e-5)\n        \n        for epoch in range(4):\n            for batch in self.loaders['forget_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                self.model.zero_grad()\n                out = self.model(ids, attention_mask=mask, labels=labels)\n                (-out.loss).backward()\n                \n                with torch.no_grad():\n                    for name, param in self.model.named_parameters():\n                        if name in retain_grad_signature and param.grad is not None:\n                            forget_grad_sign = torch.sign(param.grad.data)\n                            conflict_mask = (forget_grad_sign == retain_grad_signature[name]).float()\n                            param.grad.data *= conflict_mask\n                \n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                opt.step()\n            \n            for batch in self.loaders['retain_train']:\n                ids = batch['input_ids'].to(self.device)\n                mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                out = self.model(ids, attention_mask=mask, labels=labels)\n                with torch.no_grad():\n                    ref_out = self.ref_model(ids, attention_mask=mask)\n                \n                kl_loss = self._kl_loss(out.logits, ref_out.logits)\n                total_loss = out.loss + 0.7 * kl_loss\n                \n                opt.zero_grad()\n                total_loss.backward()\n                opt.step()\n        \n        return self.model\n\n\n# ============================================================================\n# Data Loading and Experiment Runner\n# ============================================================================\n\ndef load_data(config, tokenizer):\n    raw = load_dataset(\"ag_news\")\n    data = defaultdict(lambda: defaultdict(list))\n    for split in ['train', 'test']:\n        for item in raw[split]:\n            data[split][item['label']].append(item['text'])\n    return data\n\n\ndef make_loader(texts, labels, tokenizer, batch_size, shuffle=False):\n    ds = TextClassificationDataset(texts, labels, tokenizer, 128)\n    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n\n\ndef run_comprehensive_benchmark():\n    conf = ExperimentConfig()\n    ReportStyler.print_banner(\"Comprehensive SOTA Benchmark - Top 5 Methods\")\n    \n    print(f\"\\n{ReportStyler.BOLD}Configuration:{ReportStyler.ENDC}\")\n    print(f\"  Device: {conf.DEVICE}\")\n    print(f\"  Model: {conf.MODEL_NAME}\")\n    print(f\"  Seeds: {conf.SEEDS}\")\n    print(f\"  Forget Scenarios: {conf.FORGET_SCENARIOS}\")\n    print(f\"\\n{ReportStyler.BOLD}Top 5 SOTA Methods:{ReportStyler.ENDC}\")\n    print(f\"  1. GCM      - Gradient Conflict Minimization (H=0.9574)\")\n    print(f\"  2. PPGA     - PCA-Protected Gradient Ascent (H=0.9565)\")\n    print(f\"  3. ADU      - Alternating Direction Unlearning (H=0.9547)\")\n    print(f\"  4. CPP      - Critical Path Preservation (H=0.9538)\")\n    print(f\"  5. HFGCM    - Hybrid FFN + Gradient Conflict (H=0.9520)\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(conf.MODEL_NAME)\n    raw_data = load_data(conf, tokenizer)\n    evaluator = Evaluator(conf.DEVICE, conf)\n\n    final_results = []\n    \n    # Exactly like baseline: iterate seeds → classes → scenarios\n    for seed in conf.SEEDS:\n        set_seed(seed)\n        \n        for f_cls in range(conf.NUM_CLASSES):\n            class_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n            target_name = class_names[f_cls]\n            \n            for n_forget in conf.FORGET_SCENARIOS:\n                ReportStyler.print_scenario_header(target_name, n_forget, seed)\n                \n                # Prepare data\n                f_train_txt = raw_data['train'][f_cls][:n_forget]\n                f_train_lbl = [f_cls] * n_forget\n                \n                r_train_txt, r_train_lbl = [], []\n                for c in range(4):\n                    if c != f_cls:\n                        r_train_txt.extend(raw_data['train'][c][:conf.RETAIN_SAMPLES_PER_CLASS])\n                        r_train_lbl.extend([c] * conf.RETAIN_SAMPLES_PER_CLASS)\n                \n                t_f_txt = raw_data['test'][f_cls][:200]\n                t_f_lbl = [f_cls] * 200\n                t_r_txt, t_r_lbl = [], []\n                for c in range(4):\n                    if c != f_cls:\n                        t_r_txt.extend(raw_data['test'][c][:200])\n                        t_r_lbl.extend([c] * 200)\n\n                loaders = {\n                    'forget_train': make_loader(f_train_txt, f_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                    'retain_train': make_loader(r_train_txt, r_train_lbl, tokenizer, conf.BATCH_SIZE, True),\n                    'forget_test': make_loader(t_f_txt, t_f_lbl, tokenizer, conf.BATCH_SIZE, False),\n                    'retain_test': make_loader(t_r_txt, t_r_lbl, tokenizer, conf.BATCH_SIZE, False),\n                    'test_all': make_loader(t_f_txt + t_r_txt, t_f_lbl + t_r_lbl, tokenizer, conf.BATCH_SIZE, False)\n                }\n\n                # Pre-trained baseline\n                model = AutoModelForSequenceClassification.from_pretrained(conf.MODEL_NAME, num_labels=4).to(conf.DEVICE)\n                base_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n                ReportStyler.print_baseline_row(\"PRE-TRAINED\", base_metrics['f_acc'], base_metrics['r_acc'], base_metrics['all_acc'], base_metrics['ece'], base_metrics['mia_auc'])\n\n                # Fine-tuning\n                model.train()\n                opt = torch.optim.AdamW(model.parameters(), lr=conf.LEARNING_RATE)\n                combined = ConcatDataset([loaders['forget_train'].dataset, loaders['retain_train'].dataset])\n                train_l = DataLoader(combined, batch_size=conf.BATCH_SIZE, shuffle=True)\n                \n                for _ in range(conf.FINETUNE_EPOCHS):\n                    for batch in train_l:\n                        out = model(batch['input_ids'].to(conf.DEVICE), attention_mask=batch['attention_mask'].to(conf.DEVICE), labels=batch['labels'].to(conf.DEVICE))\n                        opt.zero_grad()\n                        out.loss.backward()\n                        opt.step()\n                \n                ft_metrics = evaluator.get_full_metrics(model, loaders, f_cls)\n                ReportStyler.print_baseline_row(\"FINE-TUNED\", ft_metrics['f_acc'], ft_metrics['r_acc'], ft_metrics['all_acc'], ft_metrics['ece'], ft_metrics['mia_auc'])\n                \n                if ft_metrics['f_acc'] < 0.7:\n                    print(f\"{ReportStyler.FAIL}Skipping: Model failed to learn target class.{ReportStyler.ENDC}\")\n                    del model\n                    clear_memory()\n                    continue\n\n                # Unlearning with Top 5 methods\n                ReportStyler.print_table_header()\n                \n                methods = [\n                    (\"GCM\", GradientConflictMinimization),\n                    (\"PPGA\", PCAProtectedGradientAscent),\n                    (\"ADU\", AlternatingDirectionUnlearning),\n                    (\"CPP\", CriticalPathPreservation),\n                    (\"HFGCM\", HybridFFNGradientConflict),\n                ]\n\n                ft_model_cpu = safe_deepcopy_model(model).to('cpu')\n\n                for name, Cls in methods:\n                    try:\n                        current_model = safe_deepcopy_model(ft_model_cpu).to(conf.DEVICE)\n                        \n                        t0 = time.time()\n                        unlearner = Cls(current_model, conf.DEVICE, loaders, conf, f_cls)\n                        unlearned_model = unlearner.run()\n                        dur = time.time() - t0\n                        \n                        res = evaluator.get_full_metrics(unlearned_model, loaders, f_cls)\n                        \n                        ReportStyler.print_result_row(\n                            name, res['f_acc'], res['r_acc'], res['all_acc'],\n                            res['ece'], res['mia_auc'], res['privacy_score'],\n                            res['h_score'], dur\n                        )\n                        \n                        final_results.append({\n                            'seed': seed, 'class': target_name, 'n': n_forget, 'method': name,\n                            **res, 'time': dur\n                        })\n                        \n                        del unlearned_model, unlearner, current_model\n                        clear_memory()\n                        \n                    except Exception as e:\n                        print(f\"│ {ReportStyler.FAIL}{name:<10} │ ERROR: {str(e)[:50]}{ReportStyler.ENDC}\")\n                        import traceback\n                        traceback.print_exc()\n\n                print(f\"│ {'─'*138}\")\n                del model, ft_model_cpu\n                clear_memory()\n\n    # ========================================================================\n    # FINAL STATISTICAL SUMMARY (Like baseline)\n    # ========================================================================\n    \n    print(\"\\n\" + \"=\" * 140)\n    print(f\"{ReportStyler.BOLD}COMPREHENSIVE BENCHMARK COMPLETE{ReportStyler.ENDC}\")\n    print(\"=\" * 140)\n    \n    if final_results:\n        # Aggregate statistics\n        method_stats = defaultdict(lambda: {'f_acc': [], 'r_acc': [], 'h_score': [], 'privacy': [], 'time': []})\n        \n        for r in final_results:\n            m = r['method']\n            method_stats[m]['f_acc'].append(r['f_acc'])\n            method_stats[m]['r_acc'].append(r['r_acc'])\n            method_stats[m]['h_score'].append(r['h_score'])\n            method_stats[m]['privacy'].append(r['privacy_score'])\n            method_stats[m]['time'].append(r['time'])\n        \n        print(f\"\\n{ReportStyler.BOLD}AGGREGATED RESULTS (Mean ± Std across all seeds/classes/scenarios):{ReportStyler.ENDC}\")\n        print(f\"\\n{'Method':<10} │ {'F-Acc':<16} │ {'R-Acc':<16} │ {'H-Score':<16} │ {'Privacy':<16} │ {'Time':<10}\")\n        print(\"-\" * 100)\n        \n        ranking = []\n        for method, stats in sorted(method_stats.items()):\n            f_mean, f_std = np.mean(stats['f_acc']), np.std(stats['f_acc'])\n            r_mean, r_std = np.mean(stats['r_acc']), np.std(stats['r_acc'])\n            h_mean, h_std = np.mean(stats['h_score']), np.std(stats['h_score'])\n            p_mean, p_std = np.mean(stats['privacy']), np.std(stats['privacy'])\n            t_mean = np.mean(stats['time'])\n            \n            print(f\"{method:<10} │ {f_mean:.3f}±{f_std:.3f}      │ {r_mean:.3f}±{r_std:.3f}      │ {h_mean:.3f}±{h_std:.3f}      │ {p_mean:.3f}±{p_std:.3f}      │ {t_mean:.1f}s\")\n            ranking.append((method, h_mean, f_mean, r_mean, p_mean, t_mean))\n        \n        ranking.sort(key=lambda x: -x[1])\n        \n        print(\"\\n\" + \"=\" * 140)\n        print(f\"{ReportStyler.BOLD}FINAL RANKING (by mean H-Score):{ReportStyler.ENDC}\")\n        print(\"=\" * 140)\n        for rank, (method, h, f, r, p, t) in enumerate(ranking, 1):\n            print(f\"  {rank}. {method:<10} H={h:.4f} | F={f:.3f} | R={r:.3f} | Privacy={p:.3f} | Time={t:.1f}s\")\n        \n        print(\"\\n\" + \"=\" * 140)\n        print(f\"{ReportStyler.BOLD}PAPER-READY LATEX TABLE:{ReportStyler.ENDC}\")\n        print(\"=\" * 140)\n        print(\"\\\\begin{table}[h]\")\n        print(\"\\\\centering\")\n        print(\"\\\\caption{Top 5 SOTA Unlearning Methods - Comprehensive Benchmark}\")\n        print(\"\\\\begin{tabular}{l|cccc}\")\n        print(\"\\\\hline\")\n        print(\"Method & F-Acc $\\\\downarrow$ & R-Acc $\\\\uparrow$ & H-Score $\\\\uparrow$ & Time (s) \\\\\\\\\")\n        print(\"\\\\hline\")\n        for method, h, f, r, p, t in ranking:\n            print(f\"{method} & {f:.3f} & {r:.3f} & \\\\textbf{{{h:.4f}}} & {t:.1f} \\\\\\\\\")\n        print(\"\\\\hline\")\n        print(\"\\\\end{tabular}\")\n        print(\"\\\\end{table}\")\n        \n        print(\"\\n\" + \"=\" * 140)\n\n    return final_results\n\n\nif __name__ == \"__main__\":\n    results = run_comprehensive_benchmark()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T00:17:51.936180Z","iopub.execute_input":"2025-12-30T00:17:51.936528Z","iopub.status.idle":"2025-12-30T01:24:00.139950Z","shell.execute_reply.started":"2025-12-30T00:17:51.936498Z","shell.execute_reply":"2025-12-30T01:24:00.139108Z"}},"outputs":[{"name":"stdout","text":"\n\u001b[1m============================================================================================================================================\u001b[0m\n\u001b[1m\u001b[95m  COMPREHENSIVE SOTA BENCHMARK - TOP 5 METHODS\u001b[0m\n\u001b[1m============================================================================================================================================\u001b[0m\n\n\u001b[1mConfiguration:\u001b[0m\n  Device: cuda\n  Model: distilbert-base-uncased\n  Seeds: [42, 123, 456]\n  Forget Scenarios: [50, 100, 200]\n\n\u001b[1mTop 5 SOTA Methods:\u001b[0m\n  1. GCM      - Gradient Conflict Minimization (H=0.9574)\n  2. PPGA     - PCA-Protected Gradient Ascent (H=0.9565)\n  3. ADU      - Alternating Direction Unlearning (H=0.9547)\n  4. CPP      - Critical Path Preservation (H=0.9538)\n  5. HFGCM    - Hybrid FFN + Gradient Conflict (H=0.9520)\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.308 │ All-Acc: 0.231 │ ECE: 0.034 │ MIA: 0.378\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.615 │ R-Acc: 0.868 │ All-Acc: 0.805 │ ECE: 0.065 │ MIA: 0.196\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.005 │ R-Acc: 0.333 │ All-Acc: 0.251 │ ECE: 0.110 │ MIA: 0.664\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.790 │ R-Acc: 0.858 │ All-Acc: 0.841 │ ECE: 0.026 │ MIA: 0.342\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.868   \u001b[0m │ 0.651    │ 0.254    │ 0.004    │ 0.008    │ \u001b[1m0.930   \u001b[0m │ 29.0   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.898   \u001b[0m │ 0.674    │ 0.223    │ 0.007    │ 0.014    │ \u001b[1m0.946   \u001b[0m │ 29.0   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.828   \u001b[0m │ 0.621    │ 0.297    │ 0.000    │ 0.001    │ \u001b[1m0.906   \u001b[0m │ 5.1    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.868   \u001b[0m │ 0.651    │ 0.249    │ 0.024    │ 0.048    │ \u001b[1m0.930   \u001b[0m │ 29.1   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.855   \u001b[0m │ 0.641    │ 0.274    │ 0.008    │ 0.015    │ \u001b[1m0.922   \u001b[0m │ 26.5   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.460 │ R-Acc: 0.100 │ All-Acc: 0.190 │ ECE: 0.073 │ MIA: 0.752\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.810 │ R-Acc: 0.828 │ All-Acc: 0.824 │ ECE: 0.044 │ MIA: 0.834\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.823   \u001b[0m │ 0.618    │ 0.289    │ 0.005    │ 0.010    │ \u001b[1m0.903   \u001b[0m │ 32.3   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.878   \u001b[0m │ 0.659    │ 0.276    │ 0.004    │ 0.007    │ \u001b[1m0.935   \u001b[0m │ 32.5   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.842   \u001b[0m │ 0.631    │ 0.314    │ 0.001    │ 0.002    │ \u001b[1m0.914   \u001b[0m │ 8.5    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.872   \u001b[0m │ 0.654    │ 0.277    │ 0.004    │ 0.007    │ \u001b[1m0.931   \u001b[0m │ 32.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.872   \u001b[0m │ 0.654    │ 0.270    │ 0.006    │ 0.012    │ \u001b[1m0.931   \u001b[0m │ 29.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.050 │ R-Acc: 0.277 │ All-Acc: 0.220 │ ECE: 0.042 │ MIA: 0.118\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.805 │ R-Acc: 0.808 │ All-Acc: 0.807 │ ECE: 0.041 │ MIA: 0.390\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.802   \u001b[0m │ 0.601    │ 0.286    │ 0.009    │ 0.017    │ \u001b[1m0.890   \u001b[0m │ 27.5   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.818   \u001b[0m │ 0.614    │ 0.274    │ 0.001    │ 0.001    │ \u001b[1m0.900   \u001b[0m │ 27.3   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.790   \u001b[0m │ 0.593    │ 0.277    │ 0.000    │ 0.000    │ \u001b[1m0.883   \u001b[0m │ 3.3    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.286    │ 0.001    │ 0.002    │ \u001b[1m0.893   \u001b[0m │ 27.5   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.792   \u001b[0m │ 0.594    │ 0.299    │ 0.000    │ 0.001    │ \u001b[1m0.884   \u001b[0m │ 24.8   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.297 │ All-Acc: 0.223 │ ECE: 0.061 │ MIA: 0.106\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.940 │ R-Acc: 0.795 │ All-Acc: 0.831 │ ECE: 0.031 │ MIA: 0.603\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.772   \u001b[0m │ 0.579    │ 0.325    │ 0.008    │ 0.016    │ \u001b[1m0.871   \u001b[0m │ 29.1   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.793   \u001b[0m │ 0.595    │ 0.312    │ 0.003    │ 0.005    │ \u001b[1m0.885   \u001b[0m │ 29.0   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.822   \u001b[0m │ 0.616    │ 0.288    │ 0.000    │ 0.000    │ \u001b[1m0.902   \u001b[0m │ 5.0    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.810   \u001b[0m │ 0.608    │ 0.319    │ 0.003    │ 0.006    │ \u001b[1m0.895   \u001b[0m │ 29.0   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.777   \u001b[0m │ 0.583    │ 0.337    │ 0.012    │ 0.023    │ \u001b[1m0.874   \u001b[0m │ 26.5   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.330 │ All-Acc: 0.247 │ ECE: 0.037 │ MIA: 0.317\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.880 │ R-Acc: 0.777 │ All-Acc: 0.802 │ ECE: 0.074 │ MIA: 0.803\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.830   \u001b[0m │ 0.623    │ 0.297    │ 0.000    │ 0.001    │ \u001b[1m0.907   \u001b[0m │ 32.2   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.813   \u001b[0m │ 0.610    │ 0.303    │ 0.000    │ 0.000    │ \u001b[1m0.897   \u001b[0m │ 32.5   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.321    │ 0.000    │ 0.000    │ \u001b[1m0.893   \u001b[0m │ 8.5    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.812   \u001b[0m │ 0.609    │ 0.318    │ 0.000    │ 0.000    │ \u001b[1m0.896   \u001b[0m │ 32.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.798   \u001b[0m │ 0.599    │ 0.294    │ 0.000    │ 0.000    │ \u001b[1m0.888   \u001b[0m │ 29.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.030 │ R-Acc: 0.390 │ All-Acc: 0.300 │ ECE: 0.037 │ MIA: 0.276\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.595 │ R-Acc: 0.892 │ All-Acc: 0.818 │ ECE: 0.048 │ MIA: 0.156\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.327 │ All-Acc: 0.245 │ ECE: 0.025 │ MIA: 0.160\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.810 │ R-Acc: 0.810 │ All-Acc: 0.810 │ ECE: 0.060 │ MIA: 0.330\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.925   \u001b[0m │ 0.694    │ 0.232    │ 0.025    │ 0.049    │ \u001b[1m0.961   \u001b[0m │ 29.0   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.935   \u001b[0m │ 0.701    │ 0.242    │ 0.021    │ 0.042    │ \u001b[1m0.966   \u001b[0m │ 29.0   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.923   \u001b[0m │ 0.693    │ 0.270    │ 0.000    │ 0.000    │ \u001b[1m0.960   \u001b[0m │ 5.0    \n│ CPP        │ \u001b[92m0.010   \u001b[0m │ 0.930   \u001b[0m │ 0.700    │ 0.226    │ 0.025    │ 0.050    │ \u001b[1m0.959   \u001b[0m │ 29.0   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.917   \u001b[0m │ 0.688    │ 0.234    │ 0.041    │ 0.083    │ \u001b[1m0.957   \u001b[0m │ 26.5   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.290 │ R-Acc: 0.278 │ All-Acc: 0.281 │ ECE: 0.003 │ MIA: 0.693\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.845 │ R-Acc: 0.830 │ All-Acc: 0.834 │ ECE: 0.035 │ MIA: 0.415\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.920   \u001b[0m │ 0.690    │ 0.253    │ 0.004    │ 0.008    │ \u001b[1m0.958   \u001b[0m │ 32.2   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.915   \u001b[0m │ 0.686    │ 0.255    │ 0.003    │ 0.007    │ \u001b[1m0.956   \u001b[0m │ 32.4   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.905   \u001b[0m │ 0.679    │ 0.278    │ 0.000    │ 0.000    │ \u001b[1m0.950   \u001b[0m │ 8.5    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.915   \u001b[0m │ 0.686    │ 0.262    │ 0.001    │ 0.002    │ \u001b[1m0.956   \u001b[0m │ 32.1   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.900   \u001b[0m │ 0.675    │ 0.277    │ 0.005    │ 0.009    │ \u001b[1m0.947   \u001b[0m │ 29.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.625 │ R-Acc: 0.167 │ All-Acc: 0.281 │ ECE: 0.006 │ MIA: 0.895\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.270 │ R-Acc: 0.893 │ All-Acc: 0.738 │ ECE: 0.108 │ MIA: 0.184\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.038 │ MIA: 0.027\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.335 │ R-Acc: 0.880 │ All-Acc: 0.744 │ ECE: 0.130 │ MIA: 0.231\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=42 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 1.000 │ R-Acc: 0.003 │ All-Acc: 0.253 │ ECE: 0.039 │ MIA: 0.999\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.655 │ R-Acc: 0.877 │ All-Acc: 0.821 │ ECE: 0.046 │ MIA: 0.481\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.100 │ R-Acc: 0.327 │ All-Acc: 0.270 │ ECE: 0.001 │ MIA: 0.628\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.775 │ R-Acc: 0.803 │ All-Acc: 0.796 │ ECE: 0.058 │ MIA: 0.292\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.890   \u001b[0m │ 0.667    │ 0.227    │ 0.006    │ 0.012    │ \u001b[1m0.942   \u001b[0m │ 27.4   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.842   \u001b[0m │ 0.631    │ 0.253    │ 0.005    │ 0.009    │ \u001b[1m0.914   \u001b[0m │ 27.3   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.790   \u001b[0m │ 0.593    │ 0.305    │ 0.001    │ 0.001    │ \u001b[1m0.883   \u001b[0m │ 3.3    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.843   \u001b[0m │ 0.632    │ 0.267    │ 0.008    │ 0.016    │ \u001b[1m0.915   \u001b[0m │ 27.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.813   \u001b[0m │ 0.610    │ 0.294    │ 0.017    │ 0.033    │ \u001b[1m0.897   \u001b[0m │ 24.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.150 │ R-Acc: 0.200 │ All-Acc: 0.188 │ ECE: 0.085 │ MIA: 0.633\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.770 │ R-Acc: 0.850 │ All-Acc: 0.830 │ ECE: 0.044 │ MIA: 0.427\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.837   \u001b[0m │ 0.627    │ 0.267    │ 0.007    │ 0.014    │ \u001b[1m0.911   \u001b[0m │ 29.1   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.855   \u001b[0m │ 0.641    │ 0.266    │ 0.005    │ 0.009    │ \u001b[1m0.922   \u001b[0m │ 29.1   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.842   \u001b[0m │ 0.631    │ 0.297    │ 0.000    │ 0.000    │ \u001b[1m0.914   \u001b[0m │ 5.0    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.885   \u001b[0m │ 0.664    │ 0.253    │ 0.005    │ 0.009    │ \u001b[1m0.939   \u001b[0m │ 29.0   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.853   \u001b[0m │ 0.640    │ 0.271    │ 0.003    │ 0.007    │ \u001b[1m0.921   \u001b[0m │ 26.6   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.332 │ All-Acc: 0.249 │ ECE: 0.039 │ MIA: 0.511\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.850 │ R-Acc: 0.805 │ All-Acc: 0.816 │ ECE: 0.053 │ MIA: 0.705\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.883   \u001b[0m │ 0.662    │ 0.247    │ 0.001    │ 0.003    │ \u001b[1m0.938   \u001b[0m │ 32.2   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.858   \u001b[0m │ 0.644    │ 0.273    │ 0.002    │ 0.004    │ \u001b[1m0.924   \u001b[0m │ 32.5   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.818   \u001b[0m │ 0.614    │ 0.329    │ 0.002    │ 0.003    │ \u001b[1m0.900   \u001b[0m │ 8.5    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.847   \u001b[0m │ 0.635    │ 0.275    │ 0.003    │ 0.005    │ \u001b[1m0.917   \u001b[0m │ 32.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.878   \u001b[0m │ 0.659    │ 0.259    │ 0.001    │ 0.003    │ \u001b[1m0.935   \u001b[0m │ 29.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.170 │ R-Acc: 0.318 │ All-Acc: 0.281 │ ECE: 0.021 │ MIA: 0.546\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.835 │ R-Acc: 0.735 │ All-Acc: 0.760 │ ECE: 0.061 │ MIA: 0.419\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.778   \u001b[0m │ 0.584    │ 0.305    │ 0.011    │ 0.021    │ \u001b[1m0.875   \u001b[0m │ 27.4   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.808   \u001b[0m │ 0.606    │ 0.293    │ 0.001    │ 0.001    │ \u001b[1m0.894   \u001b[0m │ 27.4   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.783   \u001b[0m │ 0.588    │ 0.261    │ 0.000    │ 0.000    │ \u001b[1m0.879   \u001b[0m │ 3.3    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.792   \u001b[0m │ 0.594    │ 0.278    │ 0.002    │ 0.005    │ \u001b[1m0.884   \u001b[0m │ 27.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.803   \u001b[0m │ 0.603    │ 0.281    │ 0.004    │ 0.008    │ \u001b[1m0.891   \u001b[0m │ 24.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.055 │ R-Acc: 0.268 │ All-Acc: 0.215 │ ECE: 0.048 │ MIA: 0.405\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.910 │ R-Acc: 0.817 │ All-Acc: 0.840 │ ECE: 0.036 │ MIA: 0.674\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.820   \u001b[0m │ 0.615    │ 0.259    │ 0.020    │ 0.040    │ \u001b[1m0.901   \u001b[0m │ 29.1   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.813   \u001b[0m │ 0.610    │ 0.296    │ 0.003    │ 0.006    │ \u001b[1m0.897   \u001b[0m │ 29.1   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.813   \u001b[0m │ 0.610    │ 0.273    │ 0.000    │ 0.000    │ \u001b[1m0.897   \u001b[0m │ 5.1    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.820   \u001b[0m │ 0.615    │ 0.291    │ 0.002    │ 0.005    │ \u001b[1m0.901   \u001b[0m │ 29.1   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.795   \u001b[0m │ 0.596    │ 0.299    │ 0.008    │ 0.015    │ \u001b[1m0.886   \u001b[0m │ 27.3   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.295 │ R-Acc: 0.197 │ All-Acc: 0.221 │ ECE: 0.049 │ MIA: 0.757\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.940 │ R-Acc: 0.782 │ All-Acc: 0.821 │ ECE: 0.040 │ MIA: 0.898\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.797   \u001b[0m │ 0.598    │ 0.310    │ 0.003    │ 0.006    │ \u001b[1m0.887   \u001b[0m │ 32.5   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.795   \u001b[0m │ 0.596    │ 0.322    │ 0.003    │ 0.006    │ \u001b[1m0.886   \u001b[0m │ 32.6   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.788   \u001b[0m │ 0.591    │ 0.321    │ 0.000    │ 0.000    │ \u001b[1m0.882   \u001b[0m │ 8.6    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.793   \u001b[0m │ 0.595    │ 0.305    │ 0.002    │ 0.003    │ \u001b[1m0.885   \u001b[0m │ 32.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.782   \u001b[0m │ 0.586    │ 0.309    │ 0.001    │ 0.001    │ \u001b[1m0.877   \u001b[0m │ 29.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.402 │ All-Acc: 0.301 │ ECE: 0.033 │ MIA: 0.210\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.700 │ R-Acc: 0.852 │ All-Acc: 0.814 │ ECE: 0.034 │ MIA: 0.204\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.925   \u001b[0m │ 0.694    │ 0.223    │ 0.029    │ 0.057    │ \u001b[1m0.961   \u001b[0m │ 27.6   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.922   \u001b[0m │ 0.691    │ 0.212    │ 0.037    │ 0.074    │ \u001b[1m0.959   \u001b[0m │ 27.3   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.893   \u001b[0m │ 0.670    │ 0.251    │ 0.000    │ 0.001    │ \u001b[1m0.944   \u001b[0m │ 3.4    \n│ CPP        │ \u001b[92m0.005   \u001b[0m │ 0.908   \u001b[0m │ 0.682    │ 0.215    │ 0.040    │ 0.081    │ \u001b[1m0.950   \u001b[0m │ 27.5   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.915   \u001b[0m │ 0.686    │ 0.216    │ 0.054    │ 0.108    │ \u001b[1m0.956   \u001b[0m │ 24.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.325 │ R-Acc: 0.208 │ All-Acc: 0.237 │ ECE: 0.030 │ MIA: 0.713\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.710 │ R-Acc: 0.818 │ All-Acc: 0.791 │ ECE: 0.052 │ MIA: 0.349\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.925   \u001b[0m │ 0.694    │ 0.236    │ 0.014    │ 0.027    │ \u001b[1m0.961   \u001b[0m │ 29.3   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.923   \u001b[0m │ 0.693    │ 0.236    │ 0.004    │ 0.009    │ \u001b[1m0.960   \u001b[0m │ 29.1   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.865   \u001b[0m │ 0.649    │ 0.275    │ 0.000    │ 0.000    │ \u001b[1m0.928   \u001b[0m │ 5.1    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.905   \u001b[0m │ 0.679    │ 0.244    │ 0.011    │ 0.021    │ \u001b[1m0.950   \u001b[0m │ 29.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.910   \u001b[0m │ 0.682    │ 0.227    │ 0.030    │ 0.061    │ \u001b[1m0.953   \u001b[0m │ 26.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.845 │ R-Acc: 0.082 │ All-Acc: 0.273 │ ECE: 0.002 │ MIA: 0.945\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.840 │ R-Acc: 0.823 │ All-Acc: 0.828 │ ECE: 0.042 │ MIA: 0.498\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.918   \u001b[0m │ 0.689    │ 0.250    │ 0.002    │ 0.003    │ \u001b[1m0.957   \u001b[0m │ 32.4   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.920   \u001b[0m │ 0.690    │ 0.251    │ 0.002    │ 0.005    │ \u001b[1m0.958   \u001b[0m │ 32.5   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.893   \u001b[0m │ 0.670    │ 0.286    │ 0.000    │ 0.000    │ \u001b[1m0.944   \u001b[0m │ 8.6    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.912   \u001b[0m │ 0.684    │ 0.264    │ 0.002    │ 0.004    │ \u001b[1m0.954   \u001b[0m │ 32.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.895   \u001b[0m │ 0.671    │ 0.256    │ 0.019    │ 0.037    │ \u001b[1m0.945   \u001b[0m │ 29.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.030 │ MIA: 0.388\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.305 │ R-Acc: 0.843 │ All-Acc: 0.709 │ ECE: 0.143 │ MIA: 0.254\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.045 │ R-Acc: 0.337 │ All-Acc: 0.264 │ ECE: 0.004 │ MIA: 0.291\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.455 │ R-Acc: 0.905 │ All-Acc: 0.792 │ ECE: 0.079 │ MIA: 0.252\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=123 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.430 │ R-Acc: 0.113 │ All-Acc: 0.193 │ ECE: 0.085 │ MIA: 0.835\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.685 │ R-Acc: 0.863 │ All-Acc: 0.819 │ ECE: 0.052 │ MIA: 0.431\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.240 │ R-Acc: 0.372 │ All-Acc: 0.339 │ ECE: 0.067 │ MIA: 0.478\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.650 │ R-Acc: 0.843 │ All-Acc: 0.795 │ ECE: 0.064 │ MIA: 0.226\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.045 │ R-Acc: 0.317 │ All-Acc: 0.249 │ ECE: 0.025 │ MIA: 0.634\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.750 │ R-Acc: 0.857 │ All-Acc: 0.830 │ ECE: 0.054 │ MIA: 0.323\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.877   \u001b[0m │ 0.657    │ 0.239    │ 0.014    │ 0.027    │ \u001b[1m0.934   \u001b[0m │ 29.3   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.872   \u001b[0m │ 0.654    │ 0.253    │ 0.012    │ 0.023    │ \u001b[1m0.931   \u001b[0m │ 29.2   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.845   \u001b[0m │ 0.634    │ 0.267    │ 0.000    │ 0.000    │ \u001b[1m0.916   \u001b[0m │ 5.1    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.865   \u001b[0m │ 0.649    │ 0.255    │ 0.004    │ 0.009    │ \u001b[1m0.928   \u001b[0m │ 29.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.885   \u001b[0m │ 0.664    │ 0.255    │ 0.004    │ 0.008    │ \u001b[1m0.939   \u001b[0m │ 26.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'World' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.995 │ R-Acc: 0.008 │ All-Acc: 0.255 │ ECE: 0.030 │ MIA: 1.000\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.870 │ R-Acc: 0.798 │ All-Acc: 0.816 │ ECE: 0.057 │ MIA: 0.765\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.832   \u001b[0m │ 0.624    │ 0.282    │ 0.006    │ 0.012    │ \u001b[1m0.908   \u001b[0m │ 32.5   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.863   \u001b[0m │ 0.647    │ 0.261    │ 0.004    │ 0.009    │ \u001b[1m0.927   \u001b[0m │ 32.7   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.833   \u001b[0m │ 0.625    │ 0.304    │ 0.000    │ 0.001    │ \u001b[1m0.909   \u001b[0m │ 8.6    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.833   \u001b[0m │ 0.625    │ 0.266    │ 0.005    │ 0.010    │ \u001b[1m0.909   \u001b[0m │ 32.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.868   \u001b[0m │ 0.651    │ 0.273    │ 0.004    │ 0.008    │ \u001b[1m0.930   \u001b[0m │ 29.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.940 │ R-Acc: 0.010 │ All-Acc: 0.242 │ ECE: 0.031 │ MIA: 0.957\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.740 │ R-Acc: 0.818 │ All-Acc: 0.799 │ ECE: 0.054 │ MIA: 0.339\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.842   \u001b[0m │ 0.631    │ 0.275    │ 0.007    │ 0.015    │ \u001b[1m0.914   \u001b[0m │ 27.6   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.810   \u001b[0m │ 0.608    │ 0.307    │ 0.001    │ 0.002    │ \u001b[1m0.895   \u001b[0m │ 27.5   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.818   \u001b[0m │ 0.614    │ 0.268    │ 0.000    │ 0.000    │ \u001b[1m0.900   \u001b[0m │ 3.4    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.807   \u001b[0m │ 0.605    │ 0.319    │ 0.003    │ 0.006    │ \u001b[1m0.893   \u001b[0m │ 27.6   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.817   \u001b[0m │ 0.613    │ 0.283    │ 0.004    │ 0.008    │ \u001b[1m0.899   \u001b[0m │ 24.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.345 │ All-Acc: 0.259 │ ECE: 0.021 │ MIA: 0.294\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.825 │ R-Acc: 0.797 │ All-Acc: 0.804 │ ECE: 0.051 │ MIA: 0.597\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.812   \u001b[0m │ 0.609    │ 0.302    │ 0.002    │ 0.004    │ \u001b[1m0.896   \u001b[0m │ 29.3   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.833   \u001b[0m │ 0.625    │ 0.290    │ 0.000    │ 0.001    │ \u001b[1m0.909   \u001b[0m │ 29.3   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.793   \u001b[0m │ 0.595    │ 0.304    │ 0.000    │ 0.000    │ \u001b[1m0.885   \u001b[0m │ 5.2    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.785   \u001b[0m │ 0.589    │ 0.340    │ 0.000    │ 0.000    │ \u001b[1m0.880   \u001b[0m │ 29.2   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.812   \u001b[0m │ 0.609    │ 0.304    │ 0.001    │ 0.001    │ \u001b[1m0.896   \u001b[0m │ 26.7   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sports' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.100 │ R-Acc: 0.310 │ All-Acc: 0.258 │ ECE: 0.012 │ MIA: 0.528\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.900 │ R-Acc: 0.785 │ All-Acc: 0.814 │ ECE: 0.051 │ MIA: 0.921\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.800   \u001b[0m │ 0.600    │ 0.301    │ 0.004    │ 0.009    │ \u001b[1m0.889   \u001b[0m │ 32.5   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.777   \u001b[0m │ 0.583    │ 0.326    │ 0.000    │ 0.000    │ \u001b[1m0.874   \u001b[0m │ 32.8   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.815   \u001b[0m │ 0.611    │ 0.310    │ 0.000    │ 0.000    │ \u001b[1m0.898   \u001b[0m │ 8.6    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.800   \u001b[0m │ 0.600    │ 0.312    │ 0.000    │ 0.000    │ \u001b[1m0.889   \u001b[0m │ 32.4   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.823   \u001b[0m │ 0.618    │ 0.283    │ 0.000    │ 0.000    │ \u001b[1m0.903   \u001b[0m │ 29.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.333 │ All-Acc: 0.250 │ ECE: 0.039 │ MIA: 0.155\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.735 │ R-Acc: 0.882 │ All-Acc: 0.845 │ ECE: 0.059 │ MIA: 0.182\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.937   \u001b[0m │ 0.703    │ 0.239    │ 0.021    │ 0.043    │ \u001b[1m0.967   \u001b[0m │ 27.6   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.937   \u001b[0m │ 0.703    │ 0.221    │ 0.022    │ 0.043    │ \u001b[1m0.967   \u001b[0m │ 27.4   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.907   \u001b[0m │ 0.680    │ 0.251    │ 0.001    │ 0.001    │ \u001b[1m0.951   \u001b[0m │ 3.4    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.918   \u001b[0m │ 0.689    │ 0.223    │ 0.031    │ 0.063    │ \u001b[1m0.957   \u001b[0m │ 27.5   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.920   \u001b[0m │ 0.690    │ 0.240    │ 0.037    │ 0.074    │ \u001b[1m0.958   \u001b[0m │ 24.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.150 │ R-Acc: 0.317 │ All-Acc: 0.275 │ ECE: 0.010 │ MIA: 0.532\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.650 │ R-Acc: 0.862 │ All-Acc: 0.809 │ ECE: 0.067 │ MIA: 0.228\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Business' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.327 │ All-Acc: 0.245 │ ECE: 0.041 │ MIA: 0.642\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.920 │ R-Acc: 0.792 │ All-Acc: 0.824 │ ECE: 0.054 │ MIA: 0.486\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ \u001b[1mMETHOD     │ F-ACC    │ R-ACC    │ ALL-ACC  │ ECE      │ MIA-AUC  │ PRIVACY  │ H-SCORE  │ TIME   \u001b[0m\n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n│ GCM        │ \u001b[92m0.000   \u001b[0m │ 0.917   \u001b[0m │ 0.688    │ 0.244    │ 0.036    │ 0.071    │ \u001b[1m0.957   \u001b[0m │ 32.4   \n│ PPGA       │ \u001b[92m0.000   \u001b[0m │ 0.933   \u001b[0m │ 0.700    │ 0.233    │ 0.013    │ 0.027    │ \u001b[1m0.966   \u001b[0m │ 32.7   \n│ ADU        │ \u001b[92m0.000   \u001b[0m │ 0.913   \u001b[0m │ 0.685    │ 0.279    │ 0.000    │ 0.000    │ \u001b[1m0.955   \u001b[0m │ 8.6    \n│ CPP        │ \u001b[92m0.000   \u001b[0m │ 0.912   \u001b[0m │ 0.684    │ 0.258    │ 0.011    │ 0.021    │ \u001b[1m0.954   \u001b[0m │ 32.3   \n│ HFGCM      │ \u001b[92m0.000   \u001b[0m │ 0.915   \u001b[0m │ 0.686    │ 0.232    │ 0.028    │ 0.055    │ \u001b[1m0.956   \u001b[0m │ 29.9   \n│ ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=50 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.265 │ R-Acc: 0.118 │ All-Acc: 0.155 │ ECE: 0.114 │ MIA: 0.763\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.300 │ R-Acc: 0.893 │ All-Acc: 0.745 │ ECE: 0.112 │ MIA: 0.176\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=100 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.585 │ R-Acc: 0.272 │ All-Acc: 0.350 │ ECE: 0.081 │ MIA: 0.736\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.305 │ R-Acc: 0.878 │ All-Acc: 0.735 │ ECE: 0.131 │ MIA: 0.309\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n\u001b[96m┌── SCENARIO: Forget 'Sci/Tech' | N=200 | Seed=456 ────────────────────────────────────────────────────────────────────────────────\u001b[0m\n│ \u001b[93mPRE-TRAINED \u001b[0m │ F-Acc: 0.000 │ R-Acc: 0.383 │ All-Acc: 0.287 │ ECE: 0.011 │ MIA: 0.220\n│ \u001b[93mFINE-TUNED  \u001b[0m │ F-Acc: 0.555 │ R-Acc: 0.887 │ All-Acc: 0.804 │ ECE: 0.069 │ MIA: 0.451\n\u001b[91mSkipping: Model failed to learn target class.\u001b[0m\n\n============================================================================================================================================\n\u001b[1mCOMPREHENSIVE BENCHMARK COMPLETE\u001b[0m\n============================================================================================================================================\n\n\u001b[1mAGGREGATED RESULTS (Mean ± Std across all seeds/classes/scenarios):\u001b[0m\n\nMethod     │ F-Acc            │ R-Acc            │ H-Score          │ Privacy          │ Time      \n----------------------------------------------------------------------------------------------------\nADU        │ 0.000±0.000      │ 0.840±0.044      │ 0.913±0.026      │ 0.000±0.001      │ 6.0s\nCPP        │ 0.001±0.002      │ 0.854±0.048      │ 0.920±0.027      │ 0.016±0.022      │ 29.9s\nGCM        │ 0.000±0.000      │ 0.858±0.053      │ 0.923±0.031      │ 0.020±0.019      │ 30.0s\nHFGCM      │ 0.000±0.000      │ 0.852±0.049      │ 0.919±0.028      │ 0.025±0.030      │ 27.4s\nPPGA       │ 0.000±0.000      │ 0.861±0.052      │ 0.925±0.030      │ 0.013±0.018      │ 30.0s\n\n============================================================================================================================================\n\u001b[1mFINAL RANKING (by mean H-Score):\u001b[0m\n============================================================================================================================================\n  1. PPGA       H=0.9247 | F=0.000 | R=0.861 | Privacy=0.013 | Time=30.0s\n  2. GCM        H=0.9226 | F=0.000 | R=0.858 | Privacy=0.020 | Time=30.0s\n  3. CPP        H=0.9201 | F=0.001 | R=0.854 | Privacy=0.016 | Time=29.9s\n  4. HFGCM      H=0.9193 | F=0.000 | R=0.852 | Privacy=0.025 | Time=27.4s\n  5. ADU        H=0.9126 | F=0.000 | R=0.840 | Privacy=0.000 | Time=6.0s\n\n============================================================================================================================================\n\u001b[1mPAPER-READY LATEX TABLE:\u001b[0m\n============================================================================================================================================\n\\begin{table}[h]\n\\centering\n\\caption{Top 5 SOTA Unlearning Methods - Comprehensive Benchmark}\n\\begin{tabular}{l|cccc}\n\\hline\nMethod & F-Acc $\\downarrow$ & R-Acc $\\uparrow$ & H-Score $\\uparrow$ & Time (s) \\\\\n\\hline\nPPGA & 0.000 & 0.861 & \\textbf{0.9247} & 30.0 \\\\\nGCM & 0.000 & 0.858 & \\textbf{0.9226} & 30.0 \\\\\nCPP & 0.001 & 0.854 & \\textbf{0.9201} & 29.9 \\\\\nHFGCM & 0.000 & 0.852 & \\textbf{0.9193} & 27.4 \\\\\nADU & 0.000 & 0.840 & \\textbf{0.9126} & 6.0 \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n============================================================================================================================================\n","output_type":"stream"}],"execution_count":10}]}